{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>PySR searches for symbolic expressions which optimize a particular objective.</p> <p>https://github.com/MilesCranmer/PySR/assets/7593028/c8511a49-b408-488f-8f18-b1749078268f</p>"},{"location":"#pysr-high-performance-symbolic-regression-in-python-and-julia","title":"PySR: High-Performance Symbolic Regression in Python and Julia","text":"Docs Forums Paper colab demo pip conda Stats <p>If you find PySR useful, please cite the paper arXiv:2305.01582. If you've finished a project with PySR, please submit a PR to showcase your work on the research showcase page!</p> <p>Contents:</p> <ul> <li>Contributors</li> <li>Why PySR?</li> <li>Installation</li> <li>Quickstart</li> <li>\u2192 Documentation</li> </ul>"},{"location":"#contributors","title":"Contributors \u2728","text":"<p>We are eager to welcome new contributors! Check out our contributors guide for tips \ud83d\ude80. If you have an idea for a new feature, don't hesitate to share it on the issues or discussions page.</p> Mark Kittisopikul\ud83d\udcbb \ud83d\udca1 \ud83d\ude87 \ud83d\udce6 \ud83d\udce3 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f T Coxon\ud83d\udc1b \ud83d\udcbb \ud83d\udd0c \ud83d\udca1 \ud83d\ude87 \ud83d\udea7 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f \ud83d\udcd3 Dhananjay Ashok\ud83d\udcbb \ud83c\udf0d \ud83d\udca1 \ud83d\udea7 \u26a0\ufe0f Johan Bl\u00e5b\u00e4ck\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \u26a0\ufe0f \ud83d\udcd3 JuliusMartensen\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83d\udd0c \ud83d\udca1 \ud83d\ude87 \ud83d\udea7 \ud83d\udce6 \ud83d\udce3 \ud83d\udc40 \ud83d\udd27 \ud83d\udcd3 ngam\ud83d\udcbb \ud83d\ude87 \ud83d\udce6 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f Kaze Wong\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\ude87 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udcd3 Christopher Rackauckas\ud83d\udc1b \ud83d\udcbb \ud83d\udd0c \ud83d\udca1 \ud83d\ude87 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udd27 \u26a0\ufe0f \ud83d\udcd3 Patrick Kidger\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83d\udd0c \ud83d\udca1 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udd27 \u26a0\ufe0f \ud83d\udcd3 Okon Samuel\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83d\udea7 \ud83d\udca1 \ud83d\ude87 \ud83d\udc40 \u26a0\ufe0f \ud83d\udcd3 William Booth-Clibborn\ud83d\udcbb \ud83c\udf0d \ud83d\udcd6 \ud83d\udcd3 \ud83d\udea7 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f Pablo Lemos\ud83d\udc1b \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udcd3 Jerry Ling\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83c\udf0d \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udcd3 Charles Fox\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udcd3 Johann Brehmer\ud83d\udcbb \ud83d\udcd6 \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \u26a0\ufe0f \ud83d\udcd3 Marius Millea\ud83d\udcbb \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udcd3 Coba\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\udc40 \ud83d\udcd3 Pietro Monticone\ud83d\udc1b \ud83d\udcd6 \ud83d\udca1 Mateusz Kubica\ud83d\udcd6 \ud83d\udca1 Jay Wadekar\ud83d\udc1b \ud83d\udca1 \ud83d\udce3 \ud83d\udd2c Anthony Blaom, PhD\ud83d\ude87 \ud83d\udca1 \ud83d\udc40 Jgmedina95\ud83d\udc1b \ud83d\udca1 \ud83d\udc40 Michael Abbott\ud83d\udcbb \ud83d\udca1 \ud83d\udc40 \ud83d\udd27 Oscar Smith\ud83d\udcbb \ud83d\udca1 Eric Hanson\ud83d\udca1 \ud83d\udce3 \ud83d\udcd3 Henrique Becker\ud83d\udcbb \ud83d\udca1 \ud83d\udc40 qwertyjl\ud83d\udc1b \ud83d\udcd6 \ud83d\udca1 \ud83d\udcd3 Rik Huijzer\ud83d\udca1 \ud83d\ude87 Hongyu Wang\ud83d\udca1 \ud83d\udce3 \ud83d\udd2c Saurav Maheshkar\ud83d\udd27 <p>Test status</p> Linux Windows macOS Docker Conda Coverage"},{"location":"#why-pysr","title":"Why PySR?","text":"<p>PySR is an open-source tool for Symbolic Regression: a machine learning task where the goal is to find an interpretable symbolic expression that optimizes some objective.</p> <p>Over a period of several years, PySR has been engineered from the ground up to be (1) as high-performance as possible, (2) as configurable as possible, and (3) easy to use. PySR is developed alongside the Julia library SymbolicRegression.jl, which forms the powerful search engine of PySR. The details of these algorithms are described in the PySR paper.</p> <p>Symbolic regression works best on low-dimensional datasets, but one can also extend these approaches to higher-dimensional spaces by using \"Symbolic Distillation\" of Neural Networks, as explained in 2006.11287, where we apply it to N-body problems. Here, one essentially uses symbolic regression to convert a neural net to an analytic equation. Thus, these tools simultaneously present an explicit and powerful way to interpret deep neural networks.</p>"},{"location":"#installation","title":"Installation","text":"pip conda docker Everywhere (recommended) Linux and Intel-based macOS Everywhere (if all else fails)"},{"location":"#pip","title":"pip","text":"<ol> <li>Install Julia<ul> <li>Alternatively, my personal preference is to use juliaup, which performs this automatically.</li> </ul> </li> <li>Then, run: <pre><code>pip3 install -U pysr\n</code></pre></li> <li>Finally, to install Julia dependencies: <pre><code>python3 -m pysr install\n</code></pre> <p>(Alternatively, from within Python, you can call <code>import pysr; pysr.install()</code>)</p> </li> </ol>"},{"location":"#conda","title":"conda","text":"<p>The PySR build in conda includes all required dependencies, so you can install it by simply running:</p> <pre><code>conda install -c conda-forge pysr\n</code></pre> <p>from within your target conda environment. </p> <p>However, note that the conda install does not support precompilation of Julia libraries, so the start time may be slightly slower as the JIT-compilation will be running. (Once the compilation finishes, there will not be a performance difference though.)</p>"},{"location":"#docker-build","title":"docker build","text":"<ol> <li>Clone this repo.</li> <li>In the repo, run the build command with: <pre><code>docker build -t pysr .\n</code></pre></li> <li>You can then start the container with an IPython execution with: <pre><code>docker run -it --rm pysr ipython\n</code></pre></li> </ol> <p>For more details, see the docker section.</p>"},{"location":"#common-issues","title":"Common issues","text":"<p>Common issues tend to be related to Python not finding Julia. To debug this, try running <code>python3 -c 'import os; print(os.environ[\"PATH\"])'</code>. If none of these folders contain your Julia binary, then you need to add Julia's <code>bin</code> folder to your <code>PATH</code> environment variable.</p> <p>Running PySR on macOS with an M1 processor: you should use the pip version, and make sure to get the Julia binary for ARM/M-series processors.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>You might wish to try the interactive tutorial here, which uses the notebook in <code>examples/pysr_demo.ipynb</code>.</p> <p>In practice, I highly recommend using IPython rather than Jupyter, as the printing is much nicer. Below is a quick demo here which you can paste into a Python runtime. First, let's import numpy to generate some test data:</p> <pre><code>import numpy as np\nX = 2 * np.random.randn(100, 5)\ny = 2.5382 * np.cos(X[:, 3]) + X[:, 0] ** 2 - 0.5\n</code></pre> <p>We have created a dataset with 100 datapoints, with 5 features each. The relation we wish to model is \\(2.5382 \\cos(x_3) + x_0^2 - 0.5\\).</p> <p>Now, let's create a PySR model and train it. PySR's main interface is in the style of scikit-learn:</p> <pre><code>from pysr import PySRRegressor\nmodel = PySRRegressor(\nniterations=40,  # &lt; Increase me for better results\nbinary_operators=[\"+\", \"*\"],\nunary_operators=[\n\"cos\",\n\"exp\",\n\"sin\",\n\"inv(x) = 1/x\",\n# ^ Custom operator (julia syntax)\n],\nextra_sympy_mappings={\"inv\": lambda x: 1 / x},\n# ^ Define operator for SymPy as well\nloss=\"loss(prediction, target) = (prediction - target)^2\",\n# ^ Custom loss function (julia syntax)\n)\n</code></pre> <p>This will set up the model for 40 iterations of the search code, which contains hundreds of thousands of mutations and equation evaluations.</p> <p>Let's train this model on our dataset:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>Internally, this launches a Julia process which will do a multithreaded search for equations to fit the dataset.</p> <p>Equations will be printed during training, and once you are satisfied, you may quit early by hitting 'q' and then \\&lt;enter&gt;.</p> <p>After the model has been fit, you can run <code>model.predict(X)</code> to see the predictions on a given dataset using the automatically-selected expression, or, for example, <code>model.predict(X, 3)</code> to see the predictions of the 3rd equation.</p> <p>You may run:</p> <pre><code>print(model)\n</code></pre> <p>to print the learned equations:</p> <pre><code>PySRRegressor.equations_ = [\npick     score                                           equation       loss  complexity\n0        0.000000                                          4.4324794  42.354317           1\n1        1.255691                                          (x0 * x0)   3.437307           3\n2        0.011629                          ((x0 * x0) + -0.28087974)   3.358285           5\n3        0.897855                              ((x0 * x0) + cos(x3))   1.368308           6\n4        0.857018                ((x0 * x0) + (cos(x3) * 2.4566472))   0.246483           8\n5  &gt;&gt;&gt;&gt;       inf  (((cos(x3) + -0.19699033) * 2.5382123) + (x0 *...   0.000000          10\n]\n</code></pre> <p>This arrow in the <code>pick</code> column indicates which equation is currently selected by your <code>model_selection</code> strategy for prediction. (You may change <code>model_selection</code> after <code>.fit(X, y)</code> as well.)</p> <p><code>model.equations_</code> is a pandas DataFrame containing all equations, including callable format (<code>lambda_format</code>), SymPy format (<code>sympy_format</code> - which you can also get with <code>model.sympy()</code>), and even JAX and PyTorch format (both of which are differentiable - which you can get with <code>model.jax()</code> and <code>model.pytorch()</code>).</p> <p>Note that <code>PySRRegressor</code> stores the state of the last search, and will restart from where you left off the next time you call <code>.fit()</code>, assuming you have set <code>warm_start=True</code>. This will cause problems if significant changes are made to the search parameters (like changing the operators). You can run <code>model.reset()</code> to reset the state.</p> <p>You will notice that PySR will save two files: <code>hall_of_fame...csv</code> and <code>hall_of_fame...pkl</code>. The csv file is a list of equations and their losses, and the pkl file is a saved state of the model. You may load the model from the <code>pkl</code> file with:</p> <pre><code>model = PySRRegressor.from_file(\"hall_of_fame.2022-08-10_100832.281.pkl\")\n</code></pre> <p>There are several other useful features such as denoising (e.g., <code>denoising=True</code>), feature selection (e.g., <code>select_k_features=3</code>). For examples of these and other features, see the examples page. For a detailed look at more options, see the options page. You can also see the full API at this page. There are also tips for tuning PySR on this page.</p>"},{"location":"#detailed-example","title":"Detailed Example","text":"<p>The following code makes use of as many PySR features as possible. Note that is just a demonstration of features and you should not use this example as-is. For details on what each parameter does, check out the API page.</p> <pre><code>model = PySRRegressor(\nprocs=4,\npopulations=8,\n# ^ 2 populations per core, so one is always running.\npopulation_size=50,\nncyclesperiteration=500, \n# ^ Generations between migrations.\nniterations=10000000,  # Run forever\nearly_stop_condition=(\n\"stop_if(loss, complexity) = loss &lt; 1e-6 &amp;&amp; complexity &lt; 10\"\n# Stop early if we find a good and simple equation\n),\ntimeout_in_seconds=60 * 60 * 24,\n# ^ Alternatively, stop after 24 hours have passed.\nmaxsize=50,\n# ^ Allow greater complexity.\nmaxdepth=10,\n# ^ But, avoid deep nesting.\nbinary_operators=[\"*\", \"+\", \"-\", \"/\"],\nunary_operators=[\"square\", \"cube\", \"exp\", \"cos2(x)=cos(x)^2\"],\nconstraints={\n\"/\": (-1, 9),\n\"square\": 9,\n\"cube\": 9,\n\"exp\": 9,\n},\n# ^ Limit the complexity within each argument.\n# \"inv\": (-1, 9) states that the numerator has no constraint,\n# but the denominator has a max complexity of 9.\n# \"exp\": 9 simply states that `exp` can only have\n# an expression of complexity 9 as input.\nnested_constraints={\n\"square\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n\"cube\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n\"exp\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n},\n# ^ Nesting constraints on operators. For example,\n# \"square(exp(x))\" is not allowed, since \"square\": {\"exp\": 0}.\ncomplexity_of_operators={\"/\": 2, \"exp\": 3},\n# ^ Custom complexity of particular operators.\ncomplexity_of_constants=2,\n# ^ Punish constants more than variables\nselect_k_features=4,\n# ^ Train on only the 4 most important features\nprogress=True,\n# ^ Can set to false if printing to a file.\nweight_randomize=0.1,\n# ^ Randomize the tree much more frequently\ncluster_manager=None,\n# ^ Can be set to, e.g., \"slurm\", to run a slurm\n# cluster. Just launch one script from the head node.\nprecision=64,\n# ^ Higher precision calculations.\nwarm_start=True,\n# ^ Start from where left off.\nturbo=True,\n# ^ Faster evaluation (experimental)\njulia_project=None,\n# ^ Can set to the path of a folder containing the\n# \"SymbolicRegression.jl\" repo, for custom modifications.\nupdate=False,\n# ^ Don't update Julia packages\nextra_sympy_mappings={\"cos2\": lambda x: sympy.cos(x)**2},\n# extra_torch_mappings={sympy.cos: torch.cos},\n# ^ Not needed as cos already defined, but this\n# is how you define custom torch operators.\n# extra_jax_mappings={sympy.cos: \"jnp.cos\"},\n# ^ For JAX, one passes a string.\n)\n</code></pre>"},{"location":"#docker","title":"Docker","text":"<p>You can also test out PySR in Docker, without installing it locally, by running the following command in the root directory of this repo:</p> <pre><code>docker build -t pysr .\n</code></pre> <p>This builds an image called <code>pysr</code> for your system's architecture, which also contains IPython.</p> <p>You can then run this with:</p> <pre><code>docker run -it --rm -v \"$PWD:/data\" pysr ipython\n</code></pre> <p>which will link the current directory to the container's <code>/data</code> directory and then launch ipython.</p> <p>If you have issues building for your system's architecture, you can emulate another architecture by including <code>--platform linux/amd64</code>, before the <code>build</code> and <code>run</code> commands.</p>"},{"location":"_api/","title":"PySRRegressor Reference","text":"<p><code>PySRRegressor</code> has many options for controlling a symbolic regression search. Let's look at them below.</p> <p>PARAMSKEY</p>"},{"location":"_api/#pysrregressor-functions","title":"PySRRegressor Functions","text":""},{"location":"_api/#pysr.sr.PySRRegressor.fit","title":"<code>fit(X, y, Xresampled=None, weights=None, variable_names=None, X_units=None, y_units=None)</code>","text":"<p>Search for equations to fit the dataset and store them in <code>self.equations_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray | DataFrame</code> <p>Target values of shape (n_samples,) or (n_samples, n_targets). Will be cast to X's dtype if necessary.</p> required <code>Xresampled</code> <code>ndarray | DataFrame</code> <p>Resampled training data, of shape (n_resampled, n_features), to generate a denoised data on. This will be used as the training data, rather than <code>X</code>.</p> <code>None</code> <code>weights</code> <code>ndarray | DataFrame</code> <p>Weight array of the same shape as <code>y</code>. Each element is how to weight the mean-square-error loss for that particular element of <code>y</code>. Alternatively, if a custom <code>loss</code> was set, it will can be used in arbitrary ways.</p> <code>None</code> <code>variable_names</code> <code>list[str]</code> <p>A list of names for the variables, rather than \"x0\", \"x1\", etc. If <code>X</code> is a pandas dataframe, the column names will be used instead of <code>variable_names</code>. Cannot contain spaces or special characters. Avoid variable names which are also function names in <code>sympy</code>, such as \"N\".</p> <code>None</code> <code>X_units</code> <code>list[str]</code> <p>A list of units for each variable in <code>X</code>. Each unit should be a string representing a Julia expression. See DynamicQuantities.jl https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more information.</p> <code>None</code> <code>y_units</code> <code>str | list[str]</code> <p>Similar to <code>X_units</code>, but as a unit for the target variable, <code>y</code>. If <code>y</code> is a matrix, a list of units should be passed. If <code>X_units</code> is given but <code>y_units</code> is not, then <code>y_units</code> will be arbitrary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>pysr/sr.py</code> <pre><code>def fit(\nself,\nX,\ny,\nXresampled=None,\nweights=None,\nvariable_names=None,\nX_units=None,\ny_units=None,\n):\n\"\"\"\n    Search for equations to fit the dataset and store them in `self.equations_`.\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape (n_samples, n_features).\n    y : ndarray | pandas.DataFrame\n        Target values of shape (n_samples,) or (n_samples, n_targets).\n        Will be cast to X's dtype if necessary.\n    Xresampled : ndarray | pandas.DataFrame\n        Resampled training data, of shape (n_resampled, n_features),\n        to generate a denoised data on. This\n        will be used as the training data, rather than `X`.\n    weights : ndarray | pandas.DataFrame\n        Weight array of the same shape as `y`.\n        Each element is how to weight the mean-square-error loss\n        for that particular element of `y`. Alternatively,\n        if a custom `loss` was set, it will can be used\n        in arbitrary ways.\n    variable_names : list[str]\n        A list of names for the variables, rather than \"x0\", \"x1\", etc.\n        If `X` is a pandas dataframe, the column names will be used\n        instead of `variable_names`. Cannot contain spaces or special\n        characters. Avoid variable names which are also\n        function names in `sympy`, such as \"N\".\n    X_units : list[str]\n        A list of units for each variable in `X`. Each unit should be\n        a string representing a Julia expression. See DynamicQuantities.jl\n        https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more\n        information.\n    y_units : str | list[str]\n        Similar to `X_units`, but as a unit for the target variable, `y`.\n        If `y` is a matrix, a list of units should be passed. If `X_units`\n        is given but `y_units` is not, then `y_units` will be arbitrary.\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n# Init attributes that are not specified in BaseEstimator\nif self.warm_start and hasattr(self, \"raw_julia_state_\"):\npass\nelse:\nif hasattr(self, \"raw_julia_state_\"):\nwarnings.warn(\n\"The discovered expressions are being reset. \"\n\"Please set `warm_start=True` if you wish to continue \"\n\"to start a search where you left off.\",\n)\nself.equations_ = None\nself.nout_ = 1\nself.selection_mask_ = None\nself.raw_julia_state_ = None\nself.X_units_ = None\nself.y_units_ = None\nrandom_state = check_random_state(self.random_state)  # For np random\nseed = random_state.get_state()[1][0]  # For julia random\nself._setup_equation_file()\nmutated_params = self._validate_and_set_init_params()\n(\nX,\ny,\nXresampled,\nweights,\nvariable_names,\nX_units,\ny_units,\n) = self._validate_and_set_fit_params(\nX, y, Xresampled, weights, variable_names, X_units, y_units\n)\nif X.shape[0] &gt; 10000 and not self.batching:\nwarnings.warn(\n\"Note: you are running with more than 10,000 datapoints. \"\n\"You should consider turning on batching (https://astroautomata.com/PySR/options/#batching). \"\n\"You should also reconsider if you need that many datapoints. \"\n\"Unless you have a large amount of noise (in which case you \"\n\"should smooth your dataset first), generally &lt; 10,000 datapoints \"\n\"is enough to find a functional form with symbolic regression. \"\n\"More datapoints will lower the search speed.\"\n)\n# Pre transformations (feature selection and denoising)\nX, y, variable_names, X_units, y_units = self._pre_transform_training_data(\nX, y, Xresampled, variable_names, X_units, y_units, random_state\n)\n# Warn about large feature counts (still warn if feature count is large\n# after running feature selection)\nif self.n_features_in_ &gt;= 10:\nwarnings.warn(\n\"Note: you are running with 10 features or more. \"\n\"Genetic algorithms like used in PySR scale poorly with large numbers of features. \"\n\"You should run PySR for more `niterations` to ensure it can find \"\n\"the correct variables, \"\n\"or, alternatively, do a dimensionality reduction beforehand. \"\n\"For example, `X = PCA(n_components=6).fit_transform(X)`, \"\n\"using scikit-learn's `PCA` class, \"\n\"will reduce the number of features to 6 in an interpretable way, \"\n\"as each resultant feature \"\n\"will be a linear combination of the original features. \"\n)\n# Assertion checks\nuse_custom_variable_names = variable_names is not None\n# TODO: this is always true.\n_check_assertions(\nX,\nuse_custom_variable_names,\nvariable_names,\nweights,\ny,\nX_units,\ny_units,\n)\n# Initially, just save model parameters, so that\n# it can be loaded from an early exit:\nif not self.temp_equation_file:\nself._checkpoint()\n# Perform the search:\nself._run(X, y, mutated_params, weights=weights, seed=seed)\n# Then, after fit, we save again, so the pickle file contains\n# the equations:\nif not self.temp_equation_file:\nself._checkpoint()\nreturn self\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.predict","title":"<code>predict(X, index=None)</code>","text":"<p>Predict y from input X using the equation chosen by <code>model_selection</code>.</p> <p>You may see what equation is used by printing this object. X should have the same columns as the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape <code>(n_samples, n_features)</code>.</p> required <code>index</code> <code>int | list[int]</code> <p>If you want to compute the output of an expression using a particular row of <code>self.equations_</code>, you may specify the index here. For multiple output equations, you must pass a list of indices in the same order.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_predicted</code> <code>ndarray of shape (n_samples, nout_)</code> <p>Values predicted by substituting <code>X</code> into the fitted symbolic regression model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises if the <code>best_equation</code> cannot be evaluated.</p> Source code in <code>pysr/sr.py</code> <pre><code>def predict(self, X, index=None):\n\"\"\"\n    Predict y from input X using the equation chosen by `model_selection`.\n    You may see what equation is used by printing this object. X should\n    have the same columns as the training data.\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape `(n_samples, n_features)`.\n    index : int | list[int]\n        If you want to compute the output of an expression using a\n        particular row of `self.equations_`, you may specify the index here.\n        For multiple output equations, you must pass a list of indices\n        in the same order.\n    Returns\n    -------\n    y_predicted : ndarray of shape (n_samples, nout_)\n        Values predicted by substituting `X` into the fitted symbolic\n        regression model.\n    Raises\n    ------\n    ValueError\n        Raises if the `best_equation` cannot be evaluated.\n    \"\"\"\ncheck_is_fitted(\nself, attributes=[\"selection_mask_\", \"feature_names_in_\", \"nout_\"]\n)\nbest_equation = self.get_best(index=index)\n# When X is an numpy array or a pandas dataframe with a RangeIndex,\n# the self.feature_names_in_ generated during fit, for the same X,\n# will cause a warning to be thrown during _validate_data.\n# To avoid this, convert X to a dataframe, apply the selection mask,\n# and then set the column/feature_names of X to be equal to those\n# generated during fit.\nif not isinstance(X, pd.DataFrame):\nX = check_array(X)\nX = pd.DataFrame(X)\nif isinstance(X.columns, pd.RangeIndex):\nif self.selection_mask_ is not None:\n# RangeIndex enforces column order allowing columns to\n# be correctly filtered with self.selection_mask_\nX = X.iloc[:, self.selection_mask_]\nX.columns = self.feature_names_in_\n# Without feature information, CallableEquation/lambda_format equations\n# require that the column order of X matches that of the X used during\n# the fitting process. _validate_data removes this feature information\n# when it converts the dataframe to an np array. Thus, to ensure feature\n# order is preserved after conversion, the dataframe columns must be\n# reordered/reindexed to match those of the transformed (denoised and\n# feature selected) X in fit.\nX = X.reindex(columns=self.feature_names_in_)\nX = self._validate_data(X, reset=False)\ntry:\nif self.nout_ &gt; 1:\nreturn np.stack(\n[eq[\"lambda_format\"](X) for eq in best_equation], axis=1\n)\nreturn best_equation[\"lambda_format\"](X)\nexcept Exception as error:\nraise ValueError(\n\"Failed to evaluate the expression. \"\n\"If you are using a custom operator, make sure to define it in `extra_sympy_mappings`, \"\n\"e.g., `model.set_params(extra_sympy_mappings={'inv': lambda x: 1/x})`, where \"\n\"`lambda x: 1/x` is a valid SymPy function defining the operator. \"\n\"You can then run `model.refresh()` to re-load the expressions.\"\n) from error\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.from_file","title":"<code>from_file(equation_file, *, binary_operators=None, unary_operators=None, n_features_in=None, feature_names_in=None, selection_mask=None, nout=1, **pysr_kwargs)</code>  <code>classmethod</code>","text":"<p>Create a model from a saved model checkpoint or equation file.</p> <p>Parameters:</p> Name Type Description Default <code>equation_file</code> <code>str</code> <p>Path to a pickle file containing a saved model, or a csv file containing equations.</p> required <code>binary_operators</code> <code>list[str]</code> <p>The same binary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>unary_operators</code> <code>list[str]</code> <p>The same unary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>n_features_in</code> <code>int</code> <p>Number of features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>feature_names_in</code> <code>list[str]</code> <p>Names of the features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>selection_mask</code> <code>list[bool]</code> <p>If using select_k_features, you must pass <code>model.selection_mask_</code> here. Not needed if loading from a pickle file.</p> <code>None</code> <code>nout</code> <code>int</code> <p>Number of outputs of the model. Not needed if loading from a pickle file. Default is <code>1</code>.</p> <code>1</code> <code>**pysr_kwargs</code> <code>dict</code> <p>Any other keyword arguments to initialize the PySRRegressor object. These will overwrite those stored in the pickle file. Not needed if loading from a pickle file.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>model</code> <code>PySRRegressor</code> <p>The model with fitted equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>@classmethod\ndef from_file(\ncls,\nequation_file,\n*,\nbinary_operators=None,\nunary_operators=None,\nn_features_in=None,\nfeature_names_in=None,\nselection_mask=None,\nnout=1,\n**pysr_kwargs,\n):\n\"\"\"\n    Create a model from a saved model checkpoint or equation file.\n    Parameters\n    ----------\n    equation_file : str\n        Path to a pickle file containing a saved model, or a csv file\n        containing equations.\n    binary_operators : list[str]\n        The same binary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    unary_operators : list[str]\n        The same unary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    n_features_in : int\n        Number of features passed to the model.\n        Not needed if loading from a pickle file.\n    feature_names_in : list[str]\n        Names of the features passed to the model.\n        Not needed if loading from a pickle file.\n    selection_mask : list[bool]\n        If using select_k_features, you must pass `model.selection_mask_` here.\n        Not needed if loading from a pickle file.\n    nout : int\n        Number of outputs of the model.\n        Not needed if loading from a pickle file.\n        Default is `1`.\n    **pysr_kwargs : dict\n        Any other keyword arguments to initialize the PySRRegressor object.\n        These will overwrite those stored in the pickle file.\n        Not needed if loading from a pickle file.\n    Returns\n    -------\n    model : PySRRegressor\n        The model with fitted equations.\n    \"\"\"\nif os.path.splitext(equation_file)[1] != \".pkl\":\npkl_filename = _csv_filename_to_pkl_filename(equation_file)\nelse:\npkl_filename = equation_file\n# Try to load model from &lt;equation_file&gt;.pkl\nprint(f\"Checking if {pkl_filename} exists...\")\nif os.path.exists(pkl_filename):\nprint(f\"Loading model from {pkl_filename}\")\nassert binary_operators is None\nassert unary_operators is None\nassert n_features_in is None\nwith open(pkl_filename, \"rb\") as f:\nmodel = pkl.load(f)\n# Change equation_file_ to be in the same dir as the pickle file\nbase_dir = os.path.dirname(pkl_filename)\nbase_equation_file = os.path.basename(model.equation_file_)\nmodel.equation_file_ = os.path.join(base_dir, base_equation_file)\n# Update any parameters if necessary, such as\n# extra_sympy_mappings:\nmodel.set_params(**pysr_kwargs)\nif \"equations_\" not in model.__dict__ or model.equations_ is None:\nmodel.refresh()\nreturn model\n# Else, we re-create it.\nprint(\nf\"{pkl_filename} does not exist, \"\n\"so we must create the model from scratch.\"\n)\nassert binary_operators is not None or unary_operators is not None\nassert n_features_in is not None\n# TODO: copy .bkup file if exists.\nmodel = cls(\nequation_file=equation_file,\nbinary_operators=binary_operators,\nunary_operators=unary_operators,\n**pysr_kwargs,\n)\nmodel.nout_ = nout\nmodel.n_features_in_ = n_features_in\nif feature_names_in is None:\nmodel.feature_names_in_ = np.array([f\"x{i}\" for i in range(n_features_in)])\nmodel.display_feature_names_in_ = np.array(\n[f\"x{_subscriptify(i)}\" for i in range(n_features_in)]\n)\nelse:\nassert len(feature_names_in) == n_features_in\nmodel.feature_names_in_ = feature_names_in\nmodel.display_feature_names_in_ = feature_names_in\nif selection_mask is None:\nmodel.selection_mask_ = np.ones(n_features_in, dtype=bool)\nelse:\nmodel.selection_mask_ = selection_mask\nmodel.refresh(checkpoint_file=equation_file)\nreturn model\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.sympy","title":"<code>sympy(index=None)</code>","text":"<p>Return sympy representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str, list[str] of length nout_</code> <p>SymPy representation of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def sympy(self, index=None):\n\"\"\"\n    Return sympy representation of the equation(s) chosen by `model_selection`.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    Returns\n    -------\n    best_equation : str, list[str] of length nout_\n        SymPy representation of the best equation.\n    \"\"\"\nself.refresh()\nbest_equation = self.get_best(index=index)\nif self.nout_ &gt; 1:\nreturn [eq[\"sympy_format\"] for eq in best_equation]\nreturn best_equation[\"sympy_format\"]\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.latex","title":"<code>latex(index=None, precision=3)</code>","text":"<p>Return latex representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representation. Default is <code>3</code>.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str or list[str] of length nout_</code> <p>LaTeX expression of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex(self, index=None, precision=3):\n\"\"\"\n    Return latex representation of the equation(s) chosen by `model_selection`.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representation.\n        Default is `3`.\n    Returns\n    -------\n    best_equation : str or list[str] of length nout_\n        LaTeX expression of the best equation.\n    \"\"\"\nself.refresh()\nsympy_representation = self.sympy(index=index)\nif self.nout_ &gt; 1:\noutput = []\nfor s in sympy_representation:\nlatex = to_latex(s, prec=precision)\noutput.append(latex)\nreturn output\nreturn to_latex(sympy_representation, prec=precision)\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.pytorch","title":"<code>pytorch(index=None)</code>","text":"<p>Return pytorch representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a PyTorch module containing the parameters as trainable attributes. You can use the module like any other PyTorch module: <code>module(X)</code>, where <code>X</code> is a tensor with the same column ordering as trained with.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>Module</code> <p>PyTorch module representing the expression.</p> Source code in <code>pysr/sr.py</code> <pre><code>def pytorch(self, index=None):\n\"\"\"\n    Return pytorch representation of the equation(s) chosen by `model_selection`.\n    Each equation (multiple given if there are multiple outputs) is a PyTorch module\n    containing the parameters as trainable attributes. You can use the module like\n    any other PyTorch module: `module(X)`, where `X` is a tensor with the same\n    column ordering as trained with.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    Returns\n    -------\n    best_equation : torch.nn.Module\n        PyTorch module representing the expression.\n    \"\"\"\nself.set_params(output_torch_format=True)\nself.refresh()\nbest_equation = self.get_best(index=index)\nif self.nout_ &gt; 1:\nreturn [eq[\"torch_format\"] for eq in best_equation]\nreturn best_equation[\"torch_format\"]\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.jax","title":"<code>jax(index=None)</code>","text":"<p>Return jax representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a dictionary containing {\"callable\": func, \"parameters\": params}. To call <code>func</code>, pass func(X, params). This function is differentiable using <code>jax.grad</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>dict[str, Any]</code> <p>Dictionary of callable jax function in \"callable\" key, and jax array of parameters as \"parameters\" key.</p> Source code in <code>pysr/sr.py</code> <pre><code>def jax(self, index=None):\n\"\"\"\n    Return jax representation of the equation(s) chosen by `model_selection`.\n    Each equation (multiple given if there are multiple outputs) is a dictionary\n    containing {\"callable\": func, \"parameters\": params}. To call `func`, pass\n    func(X, params). This function is differentiable using `jax.grad`.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    Returns\n    -------\n    best_equation : dict[str, Any]\n        Dictionary of callable jax function in \"callable\" key,\n        and jax array of parameters as \"parameters\" key.\n    \"\"\"\nself.set_params(output_jax_format=True)\nself.refresh()\nbest_equation = self.get_best(index=index)\nif self.nout_ &gt; 1:\nreturn [eq[\"jax_format\"] for eq in best_equation]\nreturn best_equation[\"jax_format\"]\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.latex_table","title":"<code>latex_table(indices=None, precision=3, columns=['equation', 'complexity', 'loss', 'score'])</code>","text":"<p>Create a LaTeX/booktabs table for all, or some, of the equations.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int] | list[list[int]]</code> <p>If you wish to select a particular subset of equations from <code>self.equations_</code>, give the row numbers here. By default, all equations will be used. If there are multiple output features, then pass a list of lists.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representations. Default is <code>3</code>.</p> <code>3</code> <code>columns</code> <code>list[str]</code> <p>Which columns to include in the table. Default is <code>[\"equation\", \"complexity\", \"loss\", \"score\"]</code>.</p> <code>['equation', 'complexity', 'loss', 'score']</code> <p>Returns:</p> Name Type Description <code>latex_table_str</code> <code>str</code> <p>A string that will render a table in LaTeX of the equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex_table(\nself,\nindices=None,\nprecision=3,\ncolumns=[\"equation\", \"complexity\", \"loss\", \"score\"],\n):\n\"\"\"Create a LaTeX/booktabs table for all, or some, of the equations.\n    Parameters\n    ----------\n    indices : list[int] | list[list[int]]\n        If you wish to select a particular subset of equations from\n        `self.equations_`, give the row numbers here. By default,\n        all equations will be used. If there are multiple output\n        features, then pass a list of lists.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representations.\n        Default is `3`.\n    columns : list[str]\n        Which columns to include in the table.\n        Default is `[\"equation\", \"complexity\", \"loss\", \"score\"]`.\n    Returns\n    -------\n    latex_table_str : str\n        A string that will render a table in LaTeX of the equations.\n    \"\"\"\nself.refresh()\nif self.nout_ &gt; 1:\nif indices is not None:\nassert isinstance(indices, list)\nassert isinstance(indices[0], list)\nassert len(indices) == self.nout_\ngenerator_fnc = generate_multiple_tables\nelse:\nif indices is not None:\nassert isinstance(indices, list)\nassert isinstance(indices[0], int)\ngenerator_fnc = generate_single_table\ntable_string = generator_fnc(\nself.equations_, indices=indices, precision=precision, columns=columns\n)\npreamble_string = [\nr\"\\usepackage{breqn}\",\nr\"\\usepackage{booktabs}\",\n\"\",\n\"...\",\n\"\",\n]\nreturn \"\\n\".join(preamble_string + [table_string])\n</code></pre>"},{"location":"_api/#pysr.sr.PySRRegressor.refresh","title":"<code>refresh(checkpoint_file=None)</code>","text":"<p>Update self.equations_ with any new options passed.</p> <p>For example, updating <code>extra_sympy_mappings</code> will require a <code>.refresh()</code> to update the equations.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_file</code> <code>str</code> <p>Path to checkpoint hall of fame file to be loaded. The default will use the set <code>equation_file_</code>.</p> <code>None</code> Source code in <code>pysr/sr.py</code> <pre><code>def refresh(self, checkpoint_file=None):\n\"\"\"\n    Update self.equations_ with any new options passed.\n    For example, updating `extra_sympy_mappings`\n    will require a `.refresh()` to update the equations.\n    Parameters\n    ----------\n    checkpoint_file : str\n        Path to checkpoint hall of fame file to be loaded.\n        The default will use the set `equation_file_`.\n    \"\"\"\nif checkpoint_file:\nself.equation_file_ = checkpoint_file\nself.equation_file_contents_ = None\ncheck_is_fitted(self, attributes=[\"equation_file_\"])\nself.equations_ = self.get_hof()\n</code></pre>"},{"location":"api-advanced/","title":"Internal Reference","text":""},{"location":"api-advanced/#julia-interface","title":"Julia Interface","text":"<p>Functions for initializing the Julia environment and installing deps.</p>"},{"location":"api-advanced/#pysr.julia_helpers.init_julia","title":"<code>init_julia(julia_project=None, quiet=False, julia_kwargs=None, return_aux=False)</code>","text":"<p>Initialize julia binary, turning off compiled modules if needed.</p> Source code in <code>pysr/julia_helpers.py</code> <pre><code>def init_julia(julia_project=None, quiet=False, julia_kwargs=None, return_aux=False):\n\"\"\"Initialize julia binary, turning off compiled modules if needed.\"\"\"\nglobal julia_initialized\nglobal julia_kwargs_at_initialization\nglobal julia_activated_env\nif not julia_initialized:\n_check_for_conflicting_libraries()\nif julia_kwargs is None:\njulia_kwargs = {\"optimize\": 3}\nfrom julia.core import JuliaInfo, UnsupportedPythonError\n_julia_version_assertion()\nprocessed_julia_project, is_shared = _process_julia_project(julia_project)\n_set_julia_project_env(processed_julia_project, is_shared)\ntry:\ninfo = JuliaInfo.load(julia=\"julia\")\nexcept FileNotFoundError:\nenv_path = os.environ[\"PATH\"]\nraise FileNotFoundError(\nf\"Julia is not installed in your PATH. Please install Julia and add it to your PATH.\\n\\nCurrent PATH: {env_path}\",\n)\nif not info.is_pycall_built():\nraise ImportError(_import_error())\nfrom julia.core import Julia\ntry:\nJulia(**julia_kwargs)\nexcept UnsupportedPythonError:\n# Static python binary, so we turn off pre-compiled modules.\njulia_kwargs = {**julia_kwargs, \"compiled_modules\": False}\nJulia(**julia_kwargs)\nwarnings.warn(\n\"Your system's Python library is static (e.g., conda), so precompilation will be turned off. For a dynamic library, try using `pyenv` and installing with `--enable-shared`: https://github.com/pyenv/pyenv/blob/master/plugins/python-build/README.md#building-with---enable-shared.\"\n)\nusing_compiled_modules = (not \"compiled_modules\" in julia_kwargs) or julia_kwargs[\n\"compiled_modules\"\n]\nfrom julia import Main as _Main\nMain = _Main\nif julia_activated_env is None:\njulia_activated_env = processed_julia_project\nif julia_initialized and julia_kwargs_at_initialization is not None:\n# Check if the kwargs are the same as the previous initialization\ninit_set = set(julia_kwargs_at_initialization.items())\nnew_set = set(julia_kwargs.items())\nset_diff = new_set - init_set\n# Remove the `compiled_modules` key, since it is not a user-specified kwarg:\nset_diff = {k: v for k, v in set_diff if k != \"compiled_modules\"}\nif len(set_diff) &gt; 0:\nwarnings.warn(\n\"Julia has already started. The new Julia options \"\n+ str(set_diff)\n+ \" will be ignored.\"\n)\nif julia_initialized and julia_activated_env != processed_julia_project:\nMain.eval(\"using Pkg\")\nio_arg = _get_io_arg(quiet)\n# Can't pass IO to Julia call as it evaluates to PyObject, so just directly\n# use Main.eval:\nMain.eval(\nf'Pkg.activate(\"{_escape_filename(processed_julia_project)}\",'\nf\"shared = Bool({int(is_shared)}), \"\nf\"{io_arg})\"\n)\njulia_activated_env = processed_julia_project\nif not julia_initialized:\njulia_kwargs_at_initialization = julia_kwargs\njulia_initialized = True\nif return_aux:\nreturn Main, {\"compiled_modules\": using_compiled_modules}\nreturn Main\n</code></pre>"},{"location":"api-advanced/#pysr.julia_helpers.install","title":"<code>install(julia_project=None, quiet=False, precompile=None)</code>","text":"<p>Install PyCall.jl and all required dependencies for SymbolicRegression.jl.</p> <p>Also updates the local Julia registry.</p> Source code in <code>pysr/julia_helpers.py</code> <pre><code>def install(julia_project=None, quiet=False, precompile=None):  # pragma: no cover\n\"\"\"\n    Install PyCall.jl and all required dependencies for SymbolicRegression.jl.\n    Also updates the local Julia registry.\n    \"\"\"\nimport julia\n_julia_version_assertion()\n# Set JULIA_PROJECT so that we install in the pysr environment\nprocessed_julia_project, is_shared = _process_julia_project(julia_project)\n_set_julia_project_env(processed_julia_project, is_shared)\nif precompile == False:\nos.environ[\"JULIA_PKG_PRECOMPILE_AUTO\"] = \"0\"\ntry:\njulia.install(quiet=quiet)\nexcept julia.tools.PyCallInstallError:\n# Attempt to reset PyCall.jl's build:\nsubprocess.run(\n[\n\"julia\",\n\"-e\",\nf'ENV[\"PYTHON\"] = \"{sys.executable}\"; import Pkg; Pkg.build(\"PyCall\")',\n],\n)\n# Try installing again:\njulia.install(quiet=quiet)\nMain, init_log = init_julia(julia_project, quiet=quiet, return_aux=True)\nio_arg = _get_io_arg(quiet)\nif precompile is None:\nprecompile = init_log[\"compiled_modules\"]\nif not precompile:\nMain.eval('ENV[\"JULIA_PKG_PRECOMPILE_AUTO\"] = 0')\nif is_shared:\n# Install SymbolicRegression.jl:\n_add_sr_to_julia_project(Main, io_arg)\nMain.eval(\"using Pkg\")\nMain.eval(f\"Pkg.instantiate({io_arg})\")\nif precompile:\nMain.eval(f\"Pkg.precompile({io_arg})\")\nif not quiet:\nwarnings.warn(\n\"It is recommended to restart Python after installing PySR's dependencies,\"\n\" so that the Julia environment is properly initialized.\"\n)\n</code></pre>"},{"location":"api-advanced/#exporting-to-latex","title":"Exporting to LaTeX","text":"<p>Functions to help export PySR equations to LaTeX.</p>"},{"location":"api-advanced/#pysr.export_latex.to_latex","title":"<code>to_latex(expr, prec=3, full_prec=True, **settings)</code>","text":"<p>Convert sympy expression to LaTeX with custom precision.</p> Source code in <code>pysr/export_latex.py</code> <pre><code>def to_latex(expr, prec=3, full_prec=True, **settings):\n\"\"\"Convert sympy expression to LaTeX with custom precision.\"\"\"\nsettings[\"full_prec\"] = full_prec\nprinter = PreciseLatexPrinter(settings=settings, prec=prec)\nreturn printer.doprint(expr)\n</code></pre>"},{"location":"api-advanced/#pysr.export_latex.generate_single_table","title":"<code>generate_single_table(equations, indices=None, precision=3, columns=['equation', 'complexity', 'loss', 'score'], max_equation_length=50, output_variable_name='y')</code>","text":"<p>Generate a booktabs-style LaTeX table for a single set of equations.</p> Source code in <code>pysr/export_latex.py</code> <pre><code>def generate_single_table(\nequations: pd.DataFrame,\nindices: List[int] = None,\nprecision: int = 3,\ncolumns=[\"equation\", \"complexity\", \"loss\", \"score\"],\nmax_equation_length: int = 50,\noutput_variable_name: str = \"y\",\n):\n\"\"\"Generate a booktabs-style LaTeX table for a single set of equations.\"\"\"\nassert isinstance(equations, pd.DataFrame)\nlatex_top, latex_bottom = generate_table_environment(columns)\nlatex_table_content = []\nif indices is None:\nindices = range(len(equations))\nfor i in indices:\nlatex_equation = to_latex(\nequations.iloc[i][\"sympy_format\"],\nprec=precision,\n)\ncomplexity = str(equations.iloc[i][\"complexity\"])\nloss = to_latex(\nsympy.Float(equations.iloc[i][\"loss\"]),\nprec=precision,\n)\nscore = to_latex(\nsympy.Float(equations.iloc[i][\"score\"]),\nprec=precision,\n)\nrow_pieces = []\nfor col in columns:\nif col == \"equation\":\nif len(latex_equation) &lt; max_equation_length:\nrow_pieces.append(\n\"$\" + output_variable_name + \" = \" + latex_equation + \"$\"\n)\nelse:\nbroken_latex_equation = \" \".join(\n[\nr\"\\begin{minipage}{0.8\\linewidth}\",\nr\"\\vspace{-1em}\",\nr\"\\begin{dmath*}\",\noutput_variable_name + \" = \" + latex_equation,\nr\"\\end{dmath*}\",\nr\"\\end{minipage}\",\n]\n)\nrow_pieces.append(broken_latex_equation)\nelif col == \"complexity\":\nrow_pieces.append(\"$\" + complexity + \"$\")\nelif col == \"loss\":\nrow_pieces.append(\"$\" + loss + \"$\")\nelif col == \"score\":\nrow_pieces.append(\"$\" + score + \"$\")\nelse:\nraise ValueError(f\"Unknown column: {col}\")\nlatex_table_content.append(\n\" &amp; \".join(row_pieces) + r\" \\\\\",\n)\nreturn \"\\n\".join([latex_top, *latex_table_content, latex_bottom])\n</code></pre>"},{"location":"api-advanced/#pysr.export_latex.generate_multiple_tables","title":"<code>generate_multiple_tables(equations, indices=None, precision=3, columns=['equation', 'complexity', 'loss', 'score'], output_variable_names=None)</code>","text":"<p>Generate multiple latex tables for a list of equation sets.</p> Source code in <code>pysr/export_latex.py</code> <pre><code>def generate_multiple_tables(\nequations: List[pd.DataFrame],\nindices: List[List[int]] = None,\nprecision: int = 3,\ncolumns=[\"equation\", \"complexity\", \"loss\", \"score\"],\noutput_variable_names: str = None,\n):\n\"\"\"Generate multiple latex tables for a list of equation sets.\"\"\"\n# TODO: Let user specify custom output variable\nlatex_tables = [\ngenerate_single_table(\nequations[i],\n(None if not indices else indices[i]),\nprecision=precision,\ncolumns=columns,\noutput_variable_name=(\n\"y_{\" + str(i) + \"}\"\nif output_variable_names is None\nelse output_variable_names[i]\n),\n)\nfor i in range(len(equations))\n]\nreturn \"\\n\\n\".join(latex_tables)\n</code></pre>"},{"location":"api-advanced/#pysr.export_latex.generate_table_environment","title":"<code>generate_table_environment(columns=['equation', 'complexity', 'loss'])</code>","text":"Source code in <code>pysr/export_latex.py</code> <pre><code>def generate_table_environment(columns=[\"equation\", \"complexity\", \"loss\"]):\nmargins = \"c\" * len(columns)\ncolumn_map = {\n\"complexity\": \"Complexity\",\n\"loss\": \"Loss\",\n\"equation\": \"Equation\",\n\"score\": \"Score\",\n}\ncolumns = [column_map[col] for col in columns]\ntop_pieces = [\nr\"\\begin{table}[h]\",\nr\"\\begin{center}\",\nr\"\\begin{tabular}{@{}\" + margins + r\"@{}}\",\nr\"\\toprule\",\n\" &amp; \".join(columns) + r\" \\\\\",\nr\"\\midrule\",\n]\nbottom_pieces = [\nr\"\\bottomrule\",\nr\"\\end{tabular}\",\nr\"\\end{center}\",\nr\"\\end{table}\",\n]\ntop_latex_table = \"\\n\".join(top_pieces)\nbottom_latex_table = \"\\n\".join(bottom_pieces)\nreturn top_latex_table, bottom_latex_table\n</code></pre>"},{"location":"api-advanced/#exporting-to-jax","title":"Exporting to JAX","text":""},{"location":"api-advanced/#pysr.export_jax.sympy2jax","title":"<code>sympy2jax(expression, symbols_in, selection=None, extra_jax_mappings=None)</code>","text":"<p>Returns a function f and its parameters; the function takes an input matrix, and a list of arguments:         f(X, parameters) where the parameters appear in the JAX equation.</p>"},{"location":"api-advanced/#pysr.export_jax.sympy2jax--examples","title":"Examples:","text":"<pre><code>Let's create a function in SymPy:\n```python\nx, y = symbols('x y')\ncosx = 1.0 * sympy.cos(x) + 3.2 * y\n```\nLet's get the JAX version. We pass the equation, and\nthe symbols required.\n```python\nf, params = sympy2jax(cosx, [x, y])\n```\nThe order you supply the symbols is the same order\nyou should supply the features when calling\nthe function `f` (shape `[nrows, nfeatures]`).\nIn this case, features=2 for x and y.\nThe `params` in this case will be\n`jnp.array([1.0, 3.2])`. You pass these parameters\nwhen calling the function, which will let you change them\nand take gradients.\n\nLet's generate some JAX data to pass:\n```python\nkey = random.PRNGKey(0)\nX = random.normal(key, (10, 2))\n```\n\nWe can call the function with:\n```python\nf(X, params)\n\n#&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n#                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n#                3.5427954 , -2.7479894 ], dtype=float32)\n```\n\nWe can take gradients with respect\nto the parameters for each row with JAX\ngradient parameters now:\n```python\njac_f = jax.jacobian(f, argnums=1)\njac_f(X, params)\n\n#&gt; DeviceArray([[ 0.49364874, -0.9692889 ],\n#               [ 0.8283714 , -0.0318858 ],\n#               [-0.7447336 , -1.8784496 ],\n#               [ 0.70755106, -0.3137085 ],\n#               [ 0.944834  ,  1.767703  ],\n#               [ 0.51673377,  1.4111717 ],\n#               [ 0.87347716, -0.52637756],\n#               [ 0.8760679 ,  1.0549792 ],\n#               [ 0.9961824 ,  0.79581654],\n#               [-0.88465923, -0.5822907 ]], dtype=float32)\n```\n\nWe can also JIT-compile our function:\n```python\ncompiled_f = jax.jit(f)\ncompiled_f(X, params)\n\n#&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n#                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n#                3.5427954 , -2.7479894 ], dtype=float32)\n```\n</code></pre> Source code in <code>pysr/export_jax.py</code> <pre><code>def sympy2jax(expression, symbols_in, selection=None, extra_jax_mappings=None):\n\"\"\"Returns a function f and its parameters;\n    the function takes an input matrix, and a list of arguments:\n            f(X, parameters)\n    where the parameters appear in the JAX equation.\n    # Examples:\n        Let's create a function in SymPy:\n        ```python\n        x, y = symbols('x y')\n        cosx = 1.0 * sympy.cos(x) + 3.2 * y\n        ```\n        Let's get the JAX version. We pass the equation, and\n        the symbols required.\n        ```python\n        f, params = sympy2jax(cosx, [x, y])\n        ```\n        The order you supply the symbols is the same order\n        you should supply the features when calling\n        the function `f` (shape `[nrows, nfeatures]`).\n        In this case, features=2 for x and y.\n        The `params` in this case will be\n        `jnp.array([1.0, 3.2])`. You pass these parameters\n        when calling the function, which will let you change them\n        and take gradients.\n        Let's generate some JAX data to pass:\n        ```python\n        key = random.PRNGKey(0)\n        X = random.normal(key, (10, 2))\n        ```\n        We can call the function with:\n        ```python\n        f(X, params)\n        #&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n        #                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n        #                3.5427954 , -2.7479894 ], dtype=float32)\n        ```\n        We can take gradients with respect\n        to the parameters for each row with JAX\n        gradient parameters now:\n        ```python\n        jac_f = jax.jacobian(f, argnums=1)\n        jac_f(X, params)\n        #&gt; DeviceArray([[ 0.49364874, -0.9692889 ],\n        #               [ 0.8283714 , -0.0318858 ],\n        #               [-0.7447336 , -1.8784496 ],\n        #               [ 0.70755106, -0.3137085 ],\n        #               [ 0.944834  ,  1.767703  ],\n        #               [ 0.51673377,  1.4111717 ],\n        #               [ 0.87347716, -0.52637756],\n        #               [ 0.8760679 ,  1.0549792 ],\n        #               [ 0.9961824 ,  0.79581654],\n        #               [-0.88465923, -0.5822907 ]], dtype=float32)\n        ```\n        We can also JIT-compile our function:\n        ```python\n        compiled_f = jax.jit(f)\n        compiled_f(X, params)\n        #&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n        #                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n        #                3.5427954 , -2.7479894 ], dtype=float32)\n        ```\n    \"\"\"\n_initialize_jax()\nglobal jax_initialized\nglobal jax\nglobal jnp\nglobal jsp\nparameters = []\nfunctional_form_text = sympy2jaxtext(\nexpression, parameters, symbols_in, extra_jax_mappings\n)\nhash_string = \"A_\" + str(abs(hash(str(expression) + str(symbols_in))))\ntext = f\"def {hash_string}(X, parameters):\\n\"\nif selection is not None:\n# Impose the feature selection:\ntext += f\"    X = X[:, {list(selection)}]\\n\"\ntext += \"    return \"\ntext += functional_form_text\nldict = {}\nexec(text, globals(), ldict)\nreturn ldict[hash_string], jnp.array(parameters)\n</code></pre>"},{"location":"api-advanced/#pysr.export_jax.sympy2jaxtext","title":"<code>sympy2jaxtext(expr, parameters, symbols_in, extra_jax_mappings=None)</code>","text":"Source code in <code>pysr/export_jax.py</code> <pre><code>def sympy2jaxtext(expr, parameters, symbols_in, extra_jax_mappings=None):\nif issubclass(expr.func, sympy.Float):\nparameters.append(float(expr))\nreturn f\"parameters[{len(parameters) - 1}]\"\nelif issubclass(expr.func, sympy.Rational):\nreturn f\"{float(expr)}\"\nelif issubclass(expr.func, sympy.Integer):\nreturn f\"{int(expr)}\"\nelif issubclass(expr.func, sympy.Symbol):\nreturn (\nf\"X[:, {[i for i in range(len(symbols_in)) if symbols_in[i] == expr][0]}]\"\n)\nif extra_jax_mappings is None:\nextra_jax_mappings = {}\ntry:\n_func = {**_jnp_func_lookup, **extra_jax_mappings}[expr.func]\nexcept KeyError:\nraise KeyError(\nf\"Function {expr.func} was not found in JAX function mappings.\"\n\"Please add it to extra_jax_mappings in the format, e.g., \"\n\"{sympy.sqrt: 'jnp.sqrt'}.\"\n)\nargs = [\nsympy2jaxtext(\narg, parameters, symbols_in, extra_jax_mappings=extra_jax_mappings\n)\nfor arg in expr.args\n]\nif _func == MUL:\nreturn \" * \".join([\"(\" + arg + \")\" for arg in args])\nif _func == ADD:\nreturn \" + \".join([\"(\" + arg + \")\" for arg in args])\nreturn f'{_func}({\", \".join(args)})'\n</code></pre>"},{"location":"api-advanced/#exporting-to-pytorch","title":"Exporting to PyTorch","text":""},{"location":"api-advanced/#pysr.export_torch.sympy2torch","title":"<code>sympy2torch(expression, symbols_in, selection=None, extra_torch_mappings=None)</code>","text":"<p>Returns a module for a given sympy expression with trainable parameters;</p> <p>This function will assume the input to the module is a matrix X, where     each column corresponds to each symbol you pass in <code>symbols_in</code>.</p> Source code in <code>pysr/export_torch.py</code> <pre><code>def sympy2torch(expression, symbols_in, selection=None, extra_torch_mappings=None):\n\"\"\"Returns a module for a given sympy expression with trainable parameters;\n    This function will assume the input to the module is a matrix X, where\n        each column corresponds to each symbol you pass in `symbols_in`.\n    \"\"\"\nglobal SingleSymPyModule\n_initialize_torch()\nreturn SingleSymPyModule(\nexpression, symbols_in, selection=selection, extra_funcs=extra_torch_mappings\n)\n</code></pre>"},{"location":"api/","title":"PySRRegressor Reference","text":"<p><code>PySRRegressor</code> has many options for controlling a symbolic regression search. Let's look at them below.</p>"},{"location":"api/#pysrregressor-parameters","title":"PySRRegressor Parameters","text":""},{"location":"api/#the-algorithm","title":"The Algorithm","text":""},{"location":"api/#creating-the-search-space","title":"Creating the Search Space","text":"<ul> <li> <p><code>binary_operators</code></p> <p>List of strings for binary operators used in the search. See the operators page for more details.</p> <p>Default: <code>[\"+\", \"-\", \"*\", \"/\"]</code></p> </li> <li> <p><code>unary_operators</code></p> <p>Operators which only take a single scalar as input. For example, <code>\"cos\"</code> or <code>\"exp\"</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>maxsize</code></p> <p>Max complexity of an equation.  </p> <p>Default: <code>20</code></p> </li> <li> <p><code>maxdepth</code></p> <p>Max depth of an equation. You can use both <code>maxsize</code> and <code>maxdepth</code>. <code>maxdepth</code> is by default not used.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#setting-the-search-size","title":"Setting the Search Size","text":"<ul> <li> <p><code>niterations</code></p> <p>Number of iterations of the algorithm to run. The best equations are printed and migrate between populations at the end of each iteration.</p> <p>Default: <code>40</code></p> </li> <li> <p><code>populations</code></p> <p>Number of populations running.</p> <p>Default: <code>15</code></p> </li> <li> <p><code>population_size</code></p> <p>Number of individuals in each population.</p> <p>Default: <code>33</code></p> </li> <li> <p><code>ncyclesperiteration</code></p> <p>Number of total mutations to run, per 10 samples of the population, per iteration.</p> <p>Default: <code>550</code></p> </li> </ul>"},{"location":"api/#the-objective","title":"The Objective","text":"<ul> <li> <p><code>loss</code></p> <p>String of Julia code specifying an elementwise loss function. Can either be a loss from LossFunctions.jl, or your own loss written as a function. Examples of custom written losses include: <code>myloss(x, y) = abs(x-y)</code> for non-weighted, or <code>myloss(x, y, w) = w*abs(x-y)</code> for weighted. The included losses include: Regression: <code>LPDistLoss{P}()</code>, <code>L1DistLoss()</code>, <code>L2DistLoss()</code> (mean square), <code>LogitDistLoss()</code>, <code>HuberLoss(d)</code>, <code>L1EpsilonInsLoss(\u03f5)</code>, <code>L2EpsilonInsLoss(\u03f5)</code>, <code>PeriodicLoss(c)</code>, <code>QuantileLoss(\u03c4)</code>. Classification: <code>ZeroOneLoss()</code>, <code>PerceptronLoss()</code>, <code>L1HingeLoss()</code>, <code>SmoothedL1HingeLoss(\u03b3)</code>, <code>ModifiedHuberLoss()</code>, <code>L2MarginLoss()</code>, <code>ExpLoss()</code>, <code>SigmoidLoss()</code>, <code>DWDMarginLoss(q)</code>.</p> <p>Default: <code>\"L2DistLoss()\"</code></p> </li> <li> <p><code>full_objective</code></p> <p>Alternatively, you can specify the full objective function as a snippet of Julia code, including any sort of custom evaluation (including symbolic manipulations beforehand), and any sort of loss function or regularizations. The default <code>full_objective</code> used in SymbolicRegression.jl is roughly equal to: <pre><code>function eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\nprediction, flag = eval_tree_array(tree, dataset.X, options)\nif !flag\nreturn L(Inf)\nend\nreturn sum((prediction .- dataset.y) .^ 2) / dataset.n\nend\n</code></pre> where the example elementwise loss is mean-squared error. You may pass a function with the same arguments as this (note that the name of the function doesn't matter). Here, both <code>prediction</code> and <code>dataset.y</code> are 1D arrays of length <code>dataset.n</code>. If using <code>batching</code>, then you should add an <code>idx</code> argument to the function, which is <code>nothing</code> for non-batched, and a 1D array of indices for batched.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>model_selection</code></p> <p>Model selection criterion when selecting a final expression from the list of best expression at each complexity. Can be <code>'accuracy'</code>, <code>'best'</code>, or <code>'score'</code>.  <code>'accuracy'</code> selects the candidate model with the lowest loss (highest accuracy). <code>'score'</code> selects the candidate model with the highest score. Score is defined as the negated derivative of the log-loss with respect to complexity - if an expression has a much better loss at a slightly higher complexity, it is preferred. <code>'best'</code> selects the candidate model with the highest score among expressions with a loss better than at least 1.5x the most accurate model.</p> <p>Default: <code>'best'</code></p> </li> <li> <p><code>dimensional_constraint_penalty</code></p> <p>Additive penalty for if dimensional analysis of an expression fails. By default, this is <code>1000.0</code>.</p> </li> </ul>"},{"location":"api/#working-with-complexities","title":"Working with Complexities","text":"<ul> <li> <p><code>parsimony</code></p> <p>Multiplicative factor for how much to punish complexity.</p> <p>Default: <code>0.0032</code></p> </li> <li> <p><code>constraints</code></p> <p>Dictionary of int (unary) or 2-tuples (binary), this enforces maxsize constraints on the individual arguments of operators. E.g., <code>'pow': (-1, 1)</code> says that power laws can have any complexity left argument, but only 1 complexity in the right argument. Use this to force more interpretable solutions.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>nested_constraints</code></p> <p>Specifies how many times a combination of operators can be nested. For example, <code>{\"sin\": {\"cos\": 0}}, \"cos\": {\"cos\": 2}}</code> specifies that <code>cos</code> may never appear within a <code>sin</code>, but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code> can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified, it is assumed that it can be nested an unlimited number of times. This requires that there is no operator which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation). For binary operators, you only need to provide a single number: both arguments are treated the same way, and the max of each argument is constrained.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>complexity_of_operators</code></p> <p>If you would like to use a complexity other than 1 for an operator, specify the complexity here. For example, <code>{\"sin\": 2, \"+\": 1}</code> would give a complexity of 2 for each use of the <code>sin</code> operator, and a complexity of 1 for each use of the <code>+</code> operator (which is the default). You may specify real numbers for a complexity, and the total complexity of a tree will be rounded to the nearest integer after computing.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>complexity_of_constants</code></p> <p>Complexity of constants. </p> <p>Default: <code>1</code></p> </li> <li> <p><code>complexity_of_variables</code></p> <p>Complexity of variables. </p> <p>Default: <code>1</code></p> </li> <li> <p><code>warmup_maxsize_by</code></p> <p>Whether to slowly increase max size from a small number up to the maxsize (if greater than 0).  If greater than 0, says the fraction of training time at which the current maxsize will reach the user-passed maxsize.</p> <p>Default: <code>0.0</code></p> </li> <li> <p><code>use_frequency</code></p> <p>Whether to measure the frequency of complexities, and use that instead of parsimony to explore equation space. Will naturally find equations of all complexities.</p> <p>Default: <code>True</code></p> </li> <li> <p><code>use_frequency_in_tournament</code></p> <p>Whether to use the frequency mentioned above in the tournament, rather than just the simulated annealing.</p> <p>Default: <code>True</code></p> </li> <li> <p><code>adaptive_parsimony_scaling</code></p> <p>If the adaptive parsimony strategy (<code>use_frequency</code> and <code>use_frequency_in_tournament</code>), this is how much to (exponentially) weight the contribution. If you find that the search is only optimizing the most complex expressions while the simpler expressions remain stagnant, you should increase this value.</p> <p>Default: <code>20.0</code></p> </li> <li> <p><code>should_simplify</code></p> <p>Whether to use algebraic simplification in the search. Note that only a few simple rules are implemented. </p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#mutations","title":"Mutations","text":"<ul> <li> <p><code>weight_add_node</code></p> <p>Relative likelihood for mutation to add a node.</p> <p>Default: <code>0.79</code></p> </li> <li> <p><code>weight_insert_node</code></p> <p>Relative likelihood for mutation to insert a node.</p> <p>Default: <code>5.1</code></p> </li> <li> <p><code>weight_delete_node</code></p> <p>Relative likelihood for mutation to delete a node.</p> <p>Default: <code>1.7</code></p> </li> <li> <p><code>weight_do_nothing</code></p> <p>Relative likelihood for mutation to leave the individual.</p> <p>Default: <code>0.21</code></p> </li> <li> <p><code>weight_mutate_constant</code></p> <p>Relative likelihood for mutation to change the constant slightly in a random direction.</p> <p>Default: <code>0.048</code></p> </li> <li> <p><code>weight_mutate_operator</code></p> <p>Relative likelihood for mutation to swap an operator.</p> <p>Default: <code>0.47</code></p> </li> <li> <p><code>weight_randomize</code></p> <p>Relative likelihood for mutation to completely delete and then randomly generate the equation</p> <p>Default: <code>0.00023</code></p> </li> <li> <p><code>weight_simplify</code></p> <p>Relative likelihood for mutation to simplify constant parts by evaluation</p> <p>Default: <code>0.0020</code></p> </li> <li> <p><code>weight_optimize</code></p> <p>Constant optimization can also be performed as a mutation, in addition to the normal strategy controlled by <code>optimize_probability</code> which happens every iteration. Using it as a mutation is useful if you want to use a large <code>ncyclesperiteration</code>, and may not optimize very often.</p> <p>Default: <code>0.0</code></p> </li> <li> <p><code>crossover_probability</code></p> <p>Absolute probability of crossover-type genetic operation, instead of a mutation.</p> <p>Default: <code>0.066</code></p> </li> <li> <p><code>annealing</code></p> <p>Whether to use annealing.  </p> <p>Default: <code>False</code></p> </li> <li> <p><code>alpha</code></p> <p>Initial temperature for simulated annealing (requires <code>annealing</code> to be <code>True</code>).</p> <p>Default: <code>0.1</code></p> </li> <li> <p><code>perturbation_factor</code></p> <p>Constants are perturbed by a max factor of (perturbation_factor*T + 1). Either multiplied by this or divided by this.</p> <p>Default: <code>0.076</code></p> </li> <li> <p><code>skip_mutation_failures</code></p> <p>Whether to skip mutation and crossover failures, rather than simply re-sampling the current member.</p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#tournament-selection","title":"Tournament Selection","text":"<ul> <li> <p><code>tournament_selection_n</code></p> <p>Number of expressions to consider in each tournament.</p> <p>Default: <code>10</code></p> </li> <li> <p><code>tournament_selection_p</code></p> <p>Probability of selecting the best expression in each tournament. The probability will decay as p*(1-p)^n for other expressions, sorted by loss.</p> <p>Default: <code>0.86</code></p> </li> </ul>"},{"location":"api/#constant-optimization","title":"Constant Optimization","text":"<ul> <li> <p><code>optimizer_algorithm</code></p> <p>Optimization scheme to use for optimizing constants. Can currently be <code>NelderMead</code> or <code>BFGS</code>.</p> <p>Default: <code>\"BFGS\"</code></p> </li> <li> <p><code>optimizer_nrestarts</code></p> <p>Number of time to restart the constants optimization process with different initial conditions.</p> <p>Default: <code>2</code></p> </li> <li> <p><code>optimize_probability</code></p> <p>Probability of optimizing the constants during a single iteration of the evolutionary algorithm.</p> <p>Default: <code>0.14</code></p> </li> <li> <p><code>optimizer_iterations</code></p> <p>Number of iterations that the constants optimizer can take.</p> <p>Default: <code>8</code></p> </li> <li> <p><code>should_optimize_constants</code></p> <p>Whether to numerically optimize constants (Nelder-Mead/Newton) at the end of each iteration. </p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#migration-between-populations","title":"Migration between Populations","text":"<ul> <li> <p><code>fraction_replaced</code></p> <p>How much of population to replace with migrating equations from other populations.</p> <p>Default: <code>0.000364</code></p> </li> <li> <p><code>fraction_replaced_hof</code></p> <p>How much of population to replace with migrating equations from hall of fame. </p> <p>Default: <code>0.035</code></p> </li> <li> <p><code>migration</code></p> <p>Whether to migrate.  </p> <p>Default: <code>True</code></p> </li> <li> <p><code>hof_migration</code></p> <p>Whether to have the hall of fame migrate.  </p> <p>Default: <code>True</code></p> </li> <li> <p><code>topn</code></p> <p>How many top individuals migrate from each population.</p> <p>Default: <code>12</code></p> </li> </ul>"},{"location":"api/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li> <p><code>denoise</code></p> <p>Whether to use a Gaussian Process to denoise the data before inputting to PySR. Can help PySR fit noisy data.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>select_k_features</code></p> <p>Whether to run feature selection in Python using random forests, before passing to the symbolic regression code. None means no feature selection; an int means select that many features.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#stopping-criteria","title":"Stopping Criteria","text":"<ul> <li> <p><code>max_evals</code></p> <p>Limits the total number of evaluations of expressions to this number.  </p> <p>Default: <code>None</code></p> </li> <li> <p><code>timeout_in_seconds</code></p> <p>Make the search return early once this many seconds have passed.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>early_stop_condition</code></p> <p>Stop the search early if this loss is reached. You may also pass a string containing a Julia function which takes a loss and complexity as input, for example: <code>\"f(loss, complexity) = (loss &lt; 0.1) &amp;&amp; (complexity &lt; 10)\"</code>.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#performance-and-parallelization","title":"Performance and Parallelization","text":"<ul> <li> <p><code>procs</code></p> <p>Number of processes (=number of populations running).</p> <p>Default: <code>cpu_count()</code></p> </li> <li> <p><code>multithreading</code></p> <p>Use multithreading instead of distributed backend. Using procs=0 will turn off both. </p> <p>Default: <code>True</code></p> </li> <li> <p><code>cluster_manager</code></p> <p>For distributed computing, this sets the job queue system. Set to one of \"slurm\", \"pbs\", \"lsf\", \"sge\", \"qrsh\", \"scyld\", or \"htc\". If set to one of these, PySR will run in distributed mode, and use <code>procs</code> to figure out how many processes to launch.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>batching</code></p> <p>Whether to compare population members on small batches during evolution. Still uses full dataset for comparing against hall of fame. </p> <p>Default: <code>False</code></p> </li> <li> <p><code>batch_size</code></p> <p>The amount of data to use if doing batching. </p> <p>Default: <code>50</code></p> </li> <li> <p><code>precision</code></p> <p>What precision to use for the data. By default this is <code>32</code> (float32), but you can select <code>64</code> or <code>16</code> as well, giving you 64 or 16 bits of floating point precision, respectively. If you pass complex data, the corresponding complex precision will be used (i.e., <code>64</code> for complex128, <code>32</code> for complex64).</p> <p>Default: <code>32</code></p> </li> <li> <p><code>fast_cycle</code></p> <p>Batch over population subsamples. This is a slightly different algorithm than regularized evolution, but does cycles 15% faster. May be algorithmically less efficient.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>turbo</code></p> <p>(Experimental) Whether to use LoopVectorization.jl to speed up the search evaluation. Certain operators may not be supported. Does not support 16-bit precision floats.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>enable_autodiff</code></p> <p>Whether to create derivative versions of operators for automatic differentiation. This is only necessary if you wish to compute the gradients of an expression within a custom loss function.</p> <p>Default: <code>False</code></p> </li> </ul>"},{"location":"api/#determinism","title":"Determinism","text":"<ul> <li> <p><code>random_state</code></p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>deterministic</code></p> <p>Make a PySR search give the same result every run. To use this, you must turn off parallelism (with <code>procs</code>=0, <code>multithreading</code>=False), and set <code>random_state</code> to a fixed seed.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>warm_start</code></p> <p>Tells fit to continue from where the last call to fit finished. If false, each call to fit will be fresh, overwriting previous results.</p> <p>Default: <code>False</code></p> </li> </ul>"},{"location":"api/#monitoring","title":"Monitoring","text":"<ul> <li> <p><code>verbosity</code></p> <p>What verbosity level to use. 0 means minimal print statements.</p> <p>Default: <code>1</code></p> </li> <li> <p><code>update_verbosity</code></p> <p>What verbosity level to use for package updates. Will take value of <code>verbosity</code> if not given.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>print_precision</code></p> <p>How many significant digits to print for floats. </p> <p>Default: <code>5</code></p> </li> <li> <p><code>progress</code></p> <p>Whether to use a progress bar instead of printing to stdout.</p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#environment","title":"Environment","text":"<ul> <li> <p><code>temp_equation_file</code></p> <p>Whether to put the hall of fame file in the temp directory. Deletion is then controlled with the <code>delete_tempfiles</code> parameter.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>tempdir</code></p> <p>directory for the temporary files. </p> <p>Default: <code>None</code></p> </li> <li> <p><code>delete_tempfiles</code></p> <p>Whether to delete the temporary files after finishing.</p> <p>Default: <code>True</code></p> </li> <li> <p><code>julia_project</code></p> <p>A Julia environment location containing a Project.toml (and potentially the source code for SymbolicRegression.jl). Default gives the Python package directory, where a Project.toml file should be present from the install.</p> </li> <li> <p><code>update</code></p> <p>Whether to automatically update Julia packages when <code>fit</code> is called. You should make sure that PySR is up-to-date itself first, as the packaged Julia packages may not necessarily include all updated dependencies.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>julia_kwargs</code></p> <p>Keyword arguments to pass to <code>julia.core.Julia(...)</code> to initialize the Julia runtime. The default, when <code>None</code>, is to set <code>threads</code> equal to <code>procs</code>, and <code>optimize</code> to 3.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#exporting-the-results","title":"Exporting the Results","text":"<ul> <li> <p><code>equation_file</code></p> <p>Where to save the files (.csv extension).</p> <p>Default: <code>None</code></p> </li> <li> <p><code>output_jax_format</code></p> <p>Whether to create a 'jax_format' column in the output, containing jax-callable functions and the default parameters in a jax array.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>output_torch_format</code></p> <p>Whether to create a 'torch_format' column in the output, containing a torch module with trainable parameters.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>extra_sympy_mappings</code></p> <p>Provides mappings between custom <code>binary_operators</code> or <code>unary_operators</code> defined in julia strings, to those same operators defined in sympy. E.G if <code>unary_operators=[\"inv(x)=1/x\"]</code>, then for the fitted model to be export to sympy, <code>extra_sympy_mappings</code> would be <code>{\"inv\": lambda x: 1/x}</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>extra_torch_mappings</code></p> <p>The same as <code>extra_jax_mappings</code> but for model export to pytorch. Note that the dictionary keys should be callable pytorch expressions. For example: <code>extra_torch_mappings={sympy.sin: torch.sin}</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>extra_jax_mappings</code></p> <p>Similar to <code>extra_sympy_mappings</code> but for model export to jax. The dictionary maps sympy functions to jax functions. For example: <code>extra_jax_mappings={sympy.sin: \"jnp.sin\"}</code> maps the <code>sympy.sin</code> function to the equivalent jax expression <code>jnp.sin</code>.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#pysrregressor-functions","title":"PySRRegressor Functions","text":""},{"location":"api/#pysr.sr.PySRRegressor.fit","title":"<code>fit(X, y, Xresampled=None, weights=None, variable_names=None, X_units=None, y_units=None)</code>","text":"<p>Search for equations to fit the dataset and store them in <code>self.equations_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray | DataFrame</code> <p>Target values of shape (n_samples,) or (n_samples, n_targets). Will be cast to X's dtype if necessary.</p> required <code>Xresampled</code> <code>ndarray | DataFrame</code> <p>Resampled training data, of shape (n_resampled, n_features), to generate a denoised data on. This will be used as the training data, rather than <code>X</code>.</p> <code>None</code> <code>weights</code> <code>ndarray | DataFrame</code> <p>Weight array of the same shape as <code>y</code>. Each element is how to weight the mean-square-error loss for that particular element of <code>y</code>. Alternatively, if a custom <code>loss</code> was set, it will can be used in arbitrary ways.</p> <code>None</code> <code>variable_names</code> <code>list[str]</code> <p>A list of names for the variables, rather than \"x0\", \"x1\", etc. If <code>X</code> is a pandas dataframe, the column names will be used instead of <code>variable_names</code>. Cannot contain spaces or special characters. Avoid variable names which are also function names in <code>sympy</code>, such as \"N\".</p> <code>None</code> <code>X_units</code> <code>list[str]</code> <p>A list of units for each variable in <code>X</code>. Each unit should be a string representing a Julia expression. See DynamicQuantities.jl https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more information.</p> <code>None</code> <code>y_units</code> <code>str | list[str]</code> <p>Similar to <code>X_units</code>, but as a unit for the target variable, <code>y</code>. If <code>y</code> is a matrix, a list of units should be passed. If <code>X_units</code> is given but <code>y_units</code> is not, then <code>y_units</code> will be arbitrary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>pysr/sr.py</code> <pre><code>def fit(\nself,\nX,\ny,\nXresampled=None,\nweights=None,\nvariable_names=None,\nX_units=None,\ny_units=None,\n):\n\"\"\"\n    Search for equations to fit the dataset and store them in `self.equations_`.\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape (n_samples, n_features).\n    y : ndarray | pandas.DataFrame\n        Target values of shape (n_samples,) or (n_samples, n_targets).\n        Will be cast to X's dtype if necessary.\n    Xresampled : ndarray | pandas.DataFrame\n        Resampled training data, of shape (n_resampled, n_features),\n        to generate a denoised data on. This\n        will be used as the training data, rather than `X`.\n    weights : ndarray | pandas.DataFrame\n        Weight array of the same shape as `y`.\n        Each element is how to weight the mean-square-error loss\n        for that particular element of `y`. Alternatively,\n        if a custom `loss` was set, it will can be used\n        in arbitrary ways.\n    variable_names : list[str]\n        A list of names for the variables, rather than \"x0\", \"x1\", etc.\n        If `X` is a pandas dataframe, the column names will be used\n        instead of `variable_names`. Cannot contain spaces or special\n        characters. Avoid variable names which are also\n        function names in `sympy`, such as \"N\".\n    X_units : list[str]\n        A list of units for each variable in `X`. Each unit should be\n        a string representing a Julia expression. See DynamicQuantities.jl\n        https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more\n        information.\n    y_units : str | list[str]\n        Similar to `X_units`, but as a unit for the target variable, `y`.\n        If `y` is a matrix, a list of units should be passed. If `X_units`\n        is given but `y_units` is not, then `y_units` will be arbitrary.\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n# Init attributes that are not specified in BaseEstimator\nif self.warm_start and hasattr(self, \"raw_julia_state_\"):\npass\nelse:\nif hasattr(self, \"raw_julia_state_\"):\nwarnings.warn(\n\"The discovered expressions are being reset. \"\n\"Please set `warm_start=True` if you wish to continue \"\n\"to start a search where you left off.\",\n)\nself.equations_ = None\nself.nout_ = 1\nself.selection_mask_ = None\nself.raw_julia_state_ = None\nself.X_units_ = None\nself.y_units_ = None\nrandom_state = check_random_state(self.random_state)  # For np random\nseed = random_state.get_state()[1][0]  # For julia random\nself._setup_equation_file()\nmutated_params = self._validate_and_set_init_params()\n(\nX,\ny,\nXresampled,\nweights,\nvariable_names,\nX_units,\ny_units,\n) = self._validate_and_set_fit_params(\nX, y, Xresampled, weights, variable_names, X_units, y_units\n)\nif X.shape[0] &gt; 10000 and not self.batching:\nwarnings.warn(\n\"Note: you are running with more than 10,000 datapoints. \"\n\"You should consider turning on batching (https://astroautomata.com/PySR/options/#batching). \"\n\"You should also reconsider if you need that many datapoints. \"\n\"Unless you have a large amount of noise (in which case you \"\n\"should smooth your dataset first), generally &lt; 10,000 datapoints \"\n\"is enough to find a functional form with symbolic regression. \"\n\"More datapoints will lower the search speed.\"\n)\n# Pre transformations (feature selection and denoising)\nX, y, variable_names, X_units, y_units = self._pre_transform_training_data(\nX, y, Xresampled, variable_names, X_units, y_units, random_state\n)\n# Warn about large feature counts (still warn if feature count is large\n# after running feature selection)\nif self.n_features_in_ &gt;= 10:\nwarnings.warn(\n\"Note: you are running with 10 features or more. \"\n\"Genetic algorithms like used in PySR scale poorly with large numbers of features. \"\n\"You should run PySR for more `niterations` to ensure it can find \"\n\"the correct variables, \"\n\"or, alternatively, do a dimensionality reduction beforehand. \"\n\"For example, `X = PCA(n_components=6).fit_transform(X)`, \"\n\"using scikit-learn's `PCA` class, \"\n\"will reduce the number of features to 6 in an interpretable way, \"\n\"as each resultant feature \"\n\"will be a linear combination of the original features. \"\n)\n# Assertion checks\nuse_custom_variable_names = variable_names is not None\n# TODO: this is always true.\n_check_assertions(\nX,\nuse_custom_variable_names,\nvariable_names,\nweights,\ny,\nX_units,\ny_units,\n)\n# Initially, just save model parameters, so that\n# it can be loaded from an early exit:\nif not self.temp_equation_file:\nself._checkpoint()\n# Perform the search:\nself._run(X, y, mutated_params, weights=weights, seed=seed)\n# Then, after fit, we save again, so the pickle file contains\n# the equations:\nif not self.temp_equation_file:\nself._checkpoint()\nreturn self\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.predict","title":"<code>predict(X, index=None)</code>","text":"<p>Predict y from input X using the equation chosen by <code>model_selection</code>.</p> <p>You may see what equation is used by printing this object. X should have the same columns as the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape <code>(n_samples, n_features)</code>.</p> required <code>index</code> <code>int | list[int]</code> <p>If you want to compute the output of an expression using a particular row of <code>self.equations_</code>, you may specify the index here. For multiple output equations, you must pass a list of indices in the same order.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_predicted</code> <code>ndarray of shape (n_samples, nout_)</code> <p>Values predicted by substituting <code>X</code> into the fitted symbolic regression model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises if the <code>best_equation</code> cannot be evaluated.</p> Source code in <code>pysr/sr.py</code> <pre><code>def predict(self, X, index=None):\n\"\"\"\n    Predict y from input X using the equation chosen by `model_selection`.\n    You may see what equation is used by printing this object. X should\n    have the same columns as the training data.\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape `(n_samples, n_features)`.\n    index : int | list[int]\n        If you want to compute the output of an expression using a\n        particular row of `self.equations_`, you may specify the index here.\n        For multiple output equations, you must pass a list of indices\n        in the same order.\n    Returns\n    -------\n    y_predicted : ndarray of shape (n_samples, nout_)\n        Values predicted by substituting `X` into the fitted symbolic\n        regression model.\n    Raises\n    ------\n    ValueError\n        Raises if the `best_equation` cannot be evaluated.\n    \"\"\"\ncheck_is_fitted(\nself, attributes=[\"selection_mask_\", \"feature_names_in_\", \"nout_\"]\n)\nbest_equation = self.get_best(index=index)\n# When X is an numpy array or a pandas dataframe with a RangeIndex,\n# the self.feature_names_in_ generated during fit, for the same X,\n# will cause a warning to be thrown during _validate_data.\n# To avoid this, convert X to a dataframe, apply the selection mask,\n# and then set the column/feature_names of X to be equal to those\n# generated during fit.\nif not isinstance(X, pd.DataFrame):\nX = check_array(X)\nX = pd.DataFrame(X)\nif isinstance(X.columns, pd.RangeIndex):\nif self.selection_mask_ is not None:\n# RangeIndex enforces column order allowing columns to\n# be correctly filtered with self.selection_mask_\nX = X.iloc[:, self.selection_mask_]\nX.columns = self.feature_names_in_\n# Without feature information, CallableEquation/lambda_format equations\n# require that the column order of X matches that of the X used during\n# the fitting process. _validate_data removes this feature information\n# when it converts the dataframe to an np array. Thus, to ensure feature\n# order is preserved after conversion, the dataframe columns must be\n# reordered/reindexed to match those of the transformed (denoised and\n# feature selected) X in fit.\nX = X.reindex(columns=self.feature_names_in_)\nX = self._validate_data(X, reset=False)\ntry:\nif self.nout_ &gt; 1:\nreturn np.stack(\n[eq[\"lambda_format\"](X) for eq in best_equation], axis=1\n)\nreturn best_equation[\"lambda_format\"](X)\nexcept Exception as error:\nraise ValueError(\n\"Failed to evaluate the expression. \"\n\"If you are using a custom operator, make sure to define it in `extra_sympy_mappings`, \"\n\"e.g., `model.set_params(extra_sympy_mappings={'inv': lambda x: 1/x})`, where \"\n\"`lambda x: 1/x` is a valid SymPy function defining the operator. \"\n\"You can then run `model.refresh()` to re-load the expressions.\"\n) from error\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.from_file","title":"<code>from_file(equation_file, *, binary_operators=None, unary_operators=None, n_features_in=None, feature_names_in=None, selection_mask=None, nout=1, **pysr_kwargs)</code>  <code>classmethod</code>","text":"<p>Create a model from a saved model checkpoint or equation file.</p> <p>Parameters:</p> Name Type Description Default <code>equation_file</code> <code>str</code> <p>Path to a pickle file containing a saved model, or a csv file containing equations.</p> required <code>binary_operators</code> <code>list[str]</code> <p>The same binary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>unary_operators</code> <code>list[str]</code> <p>The same unary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>n_features_in</code> <code>int</code> <p>Number of features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>feature_names_in</code> <code>list[str]</code> <p>Names of the features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>selection_mask</code> <code>list[bool]</code> <p>If using select_k_features, you must pass <code>model.selection_mask_</code> here. Not needed if loading from a pickle file.</p> <code>None</code> <code>nout</code> <code>int</code> <p>Number of outputs of the model. Not needed if loading from a pickle file. Default is <code>1</code>.</p> <code>1</code> <code>**pysr_kwargs</code> <code>dict</code> <p>Any other keyword arguments to initialize the PySRRegressor object. These will overwrite those stored in the pickle file. Not needed if loading from a pickle file.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>model</code> <code>PySRRegressor</code> <p>The model with fitted equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>@classmethod\ndef from_file(\ncls,\nequation_file,\n*,\nbinary_operators=None,\nunary_operators=None,\nn_features_in=None,\nfeature_names_in=None,\nselection_mask=None,\nnout=1,\n**pysr_kwargs,\n):\n\"\"\"\n    Create a model from a saved model checkpoint or equation file.\n    Parameters\n    ----------\n    equation_file : str\n        Path to a pickle file containing a saved model, or a csv file\n        containing equations.\n    binary_operators : list[str]\n        The same binary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    unary_operators : list[str]\n        The same unary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    n_features_in : int\n        Number of features passed to the model.\n        Not needed if loading from a pickle file.\n    feature_names_in : list[str]\n        Names of the features passed to the model.\n        Not needed if loading from a pickle file.\n    selection_mask : list[bool]\n        If using select_k_features, you must pass `model.selection_mask_` here.\n        Not needed if loading from a pickle file.\n    nout : int\n        Number of outputs of the model.\n        Not needed if loading from a pickle file.\n        Default is `1`.\n    **pysr_kwargs : dict\n        Any other keyword arguments to initialize the PySRRegressor object.\n        These will overwrite those stored in the pickle file.\n        Not needed if loading from a pickle file.\n    Returns\n    -------\n    model : PySRRegressor\n        The model with fitted equations.\n    \"\"\"\nif os.path.splitext(equation_file)[1] != \".pkl\":\npkl_filename = _csv_filename_to_pkl_filename(equation_file)\nelse:\npkl_filename = equation_file\n# Try to load model from &lt;equation_file&gt;.pkl\nprint(f\"Checking if {pkl_filename} exists...\")\nif os.path.exists(pkl_filename):\nprint(f\"Loading model from {pkl_filename}\")\nassert binary_operators is None\nassert unary_operators is None\nassert n_features_in is None\nwith open(pkl_filename, \"rb\") as f:\nmodel = pkl.load(f)\n# Change equation_file_ to be in the same dir as the pickle file\nbase_dir = os.path.dirname(pkl_filename)\nbase_equation_file = os.path.basename(model.equation_file_)\nmodel.equation_file_ = os.path.join(base_dir, base_equation_file)\n# Update any parameters if necessary, such as\n# extra_sympy_mappings:\nmodel.set_params(**pysr_kwargs)\nif \"equations_\" not in model.__dict__ or model.equations_ is None:\nmodel.refresh()\nreturn model\n# Else, we re-create it.\nprint(\nf\"{pkl_filename} does not exist, \"\n\"so we must create the model from scratch.\"\n)\nassert binary_operators is not None or unary_operators is not None\nassert n_features_in is not None\n# TODO: copy .bkup file if exists.\nmodel = cls(\nequation_file=equation_file,\nbinary_operators=binary_operators,\nunary_operators=unary_operators,\n**pysr_kwargs,\n)\nmodel.nout_ = nout\nmodel.n_features_in_ = n_features_in\nif feature_names_in is None:\nmodel.feature_names_in_ = np.array([f\"x{i}\" for i in range(n_features_in)])\nmodel.display_feature_names_in_ = np.array(\n[f\"x{_subscriptify(i)}\" for i in range(n_features_in)]\n)\nelse:\nassert len(feature_names_in) == n_features_in\nmodel.feature_names_in_ = feature_names_in\nmodel.display_feature_names_in_ = feature_names_in\nif selection_mask is None:\nmodel.selection_mask_ = np.ones(n_features_in, dtype=bool)\nelse:\nmodel.selection_mask_ = selection_mask\nmodel.refresh(checkpoint_file=equation_file)\nreturn model\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.sympy","title":"<code>sympy(index=None)</code>","text":"<p>Return sympy representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str, list[str] of length nout_</code> <p>SymPy representation of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def sympy(self, index=None):\n\"\"\"\n    Return sympy representation of the equation(s) chosen by `model_selection`.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    Returns\n    -------\n    best_equation : str, list[str] of length nout_\n        SymPy representation of the best equation.\n    \"\"\"\nself.refresh()\nbest_equation = self.get_best(index=index)\nif self.nout_ &gt; 1:\nreturn [eq[\"sympy_format\"] for eq in best_equation]\nreturn best_equation[\"sympy_format\"]\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.latex","title":"<code>latex(index=None, precision=3)</code>","text":"<p>Return latex representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representation. Default is <code>3</code>.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str or list[str] of length nout_</code> <p>LaTeX expression of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex(self, index=None, precision=3):\n\"\"\"\n    Return latex representation of the equation(s) chosen by `model_selection`.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representation.\n        Default is `3`.\n    Returns\n    -------\n    best_equation : str or list[str] of length nout_\n        LaTeX expression of the best equation.\n    \"\"\"\nself.refresh()\nsympy_representation = self.sympy(index=index)\nif self.nout_ &gt; 1:\noutput = []\nfor s in sympy_representation:\nlatex = to_latex(s, prec=precision)\noutput.append(latex)\nreturn output\nreturn to_latex(sympy_representation, prec=precision)\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.pytorch","title":"<code>pytorch(index=None)</code>","text":"<p>Return pytorch representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a PyTorch module containing the parameters as trainable attributes. You can use the module like any other PyTorch module: <code>module(X)</code>, where <code>X</code> is a tensor with the same column ordering as trained with.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>Module</code> <p>PyTorch module representing the expression.</p> Source code in <code>pysr/sr.py</code> <pre><code>def pytorch(self, index=None):\n\"\"\"\n    Return pytorch representation of the equation(s) chosen by `model_selection`.\n    Each equation (multiple given if there are multiple outputs) is a PyTorch module\n    containing the parameters as trainable attributes. You can use the module like\n    any other PyTorch module: `module(X)`, where `X` is a tensor with the same\n    column ordering as trained with.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    Returns\n    -------\n    best_equation : torch.nn.Module\n        PyTorch module representing the expression.\n    \"\"\"\nself.set_params(output_torch_format=True)\nself.refresh()\nbest_equation = self.get_best(index=index)\nif self.nout_ &gt; 1:\nreturn [eq[\"torch_format\"] for eq in best_equation]\nreturn best_equation[\"torch_format\"]\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.jax","title":"<code>jax(index=None)</code>","text":"<p>Return jax representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a dictionary containing {\"callable\": func, \"parameters\": params}. To call <code>func</code>, pass func(X, params). This function is differentiable using <code>jax.grad</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>dict[str, Any]</code> <p>Dictionary of callable jax function in \"callable\" key, and jax array of parameters as \"parameters\" key.</p> Source code in <code>pysr/sr.py</code> <pre><code>def jax(self, index=None):\n\"\"\"\n    Return jax representation of the equation(s) chosen by `model_selection`.\n    Each equation (multiple given if there are multiple outputs) is a dictionary\n    containing {\"callable\": func, \"parameters\": params}. To call `func`, pass\n    func(X, params). This function is differentiable using `jax.grad`.\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    Returns\n    -------\n    best_equation : dict[str, Any]\n        Dictionary of callable jax function in \"callable\" key,\n        and jax array of parameters as \"parameters\" key.\n    \"\"\"\nself.set_params(output_jax_format=True)\nself.refresh()\nbest_equation = self.get_best(index=index)\nif self.nout_ &gt; 1:\nreturn [eq[\"jax_format\"] for eq in best_equation]\nreturn best_equation[\"jax_format\"]\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.latex_table","title":"<code>latex_table(indices=None, precision=3, columns=['equation', 'complexity', 'loss', 'score'])</code>","text":"<p>Create a LaTeX/booktabs table for all, or some, of the equations.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int] | list[list[int]]</code> <p>If you wish to select a particular subset of equations from <code>self.equations_</code>, give the row numbers here. By default, all equations will be used. If there are multiple output features, then pass a list of lists.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representations. Default is <code>3</code>.</p> <code>3</code> <code>columns</code> <code>list[str]</code> <p>Which columns to include in the table. Default is <code>[\"equation\", \"complexity\", \"loss\", \"score\"]</code>.</p> <code>['equation', 'complexity', 'loss', 'score']</code> <p>Returns:</p> Name Type Description <code>latex_table_str</code> <code>str</code> <p>A string that will render a table in LaTeX of the equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex_table(\nself,\nindices=None,\nprecision=3,\ncolumns=[\"equation\", \"complexity\", \"loss\", \"score\"],\n):\n\"\"\"Create a LaTeX/booktabs table for all, or some, of the equations.\n    Parameters\n    ----------\n    indices : list[int] | list[list[int]]\n        If you wish to select a particular subset of equations from\n        `self.equations_`, give the row numbers here. By default,\n        all equations will be used. If there are multiple output\n        features, then pass a list of lists.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representations.\n        Default is `3`.\n    columns : list[str]\n        Which columns to include in the table.\n        Default is `[\"equation\", \"complexity\", \"loss\", \"score\"]`.\n    Returns\n    -------\n    latex_table_str : str\n        A string that will render a table in LaTeX of the equations.\n    \"\"\"\nself.refresh()\nif self.nout_ &gt; 1:\nif indices is not None:\nassert isinstance(indices, list)\nassert isinstance(indices[0], list)\nassert len(indices) == self.nout_\ngenerator_fnc = generate_multiple_tables\nelse:\nif indices is not None:\nassert isinstance(indices, list)\nassert isinstance(indices[0], int)\ngenerator_fnc = generate_single_table\ntable_string = generator_fnc(\nself.equations_, indices=indices, precision=precision, columns=columns\n)\npreamble_string = [\nr\"\\usepackage{breqn}\",\nr\"\\usepackage{booktabs}\",\n\"\",\n\"...\",\n\"\",\n]\nreturn \"\\n\".join(preamble_string + [table_string])\n</code></pre>"},{"location":"api/#pysr.sr.PySRRegressor.refresh","title":"<code>refresh(checkpoint_file=None)</code>","text":"<p>Update self.equations_ with any new options passed.</p> <p>For example, updating <code>extra_sympy_mappings</code> will require a <code>.refresh()</code> to update the equations.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_file</code> <code>str</code> <p>Path to checkpoint hall of fame file to be loaded. The default will use the set <code>equation_file_</code>.</p> <code>None</code> Source code in <code>pysr/sr.py</code> <pre><code>def refresh(self, checkpoint_file=None):\n\"\"\"\n    Update self.equations_ with any new options passed.\n    For example, updating `extra_sympy_mappings`\n    will require a `.refresh()` to update the equations.\n    Parameters\n    ----------\n    checkpoint_file : str\n        Path to checkpoint hall of fame file to be loaded.\n        The default will use the set `equation_file_`.\n    \"\"\"\nif checkpoint_file:\nself.equation_file_ = checkpoint_file\nself.equation_file_contents_ = None\ncheck_is_fitted(self, attributes=[\"equation_file_\"])\nself.equations_ = self.get_hof()\n</code></pre>"},{"location":"backend/","title":"Customization","text":"<p>If you have explored the options and PySRRegressor reference, and still haven't figured out how to specify a constraint or objective required for your problem, you might consider editing the backend. The backend of PySR is written as a pure Julia package under the name SymbolicRegression.jl. This package is accessed with <code>PyJulia</code>, which allows us to transfer objects back and forth between the Python and Julia runtimes.</p> <p>PySR gives you access to everything in SymbolicRegression.jl, but there are some specific use-cases which require modifications to the backend itself. Generally you can do this as follows:</p> <ol> <li>Clone a copy of the backend: <pre><code>git clone https://github.com/MilesCranmer/SymbolicRegression.jl\n</code></pre></li> <li>Edit the source code in <code>src/</code> to your requirements:<ul> <li>The documentation for the backend is given here. </li> <li>Throughout the package, you will often see template functions which typically use a symbol <code>T</code> (such as in the string <code>where {T&lt;:Real}</code>). Here, <code>T</code> is simply the datatype of the input data and stored constants, such as <code>Float32</code> or <code>Float64</code>. Writing functions in this way lets us write functions generic to types, while still having access to the specific type specified at compilation time.</li> <li>Expressions are stored as binary trees, using the <code>Node{T}</code> type, described here.</li> <li>Parts of the code which are typically edited by users include:<ul> <li><code>src/LossFunctions.jl</code>, particularly the function <code>eval_loss</code>. This function assigns a loss to a given expression, using <code>eval_tree_array</code> to evaluate it, and <code>loss</code> to compute the loss with respect to the dataset.</li> <li><code>src/CheckConstraints.jl</code>, particularly the function <code>check_constraints</code>. This function checks whether a given expression satisfies constraints, such as having a complexity lower than <code>maxsize</code>, and whether it contains any forbidden nestings of functions.<ul> <li>Note that all expressions, even intermediate expressions, must comply with constraints. Therefore, make sure that evolution can still reach your desired expression (with one mutation at a time), before setting a hard constraint. In other cases you might want to instead put in the loss function.</li> </ul> </li> <li><code>src/Options.jl</code>, as well as the struct definition in <code>src/OptionsStruct.jl</code>. This file specifies all the options used in the search: an instance of <code>Options</code> is typically available throughout every function in <code>SymbolicRegression.jl</code>. If you add new functionality to the backend, and wish to make it parameterizable (including from PySR), you should specify it in the options.</li> <li>For reference, the main loop itself is found in the <code>equation_search</code> function inside <code>src/SymbolicRegression.jl</code>.</li> </ul> </li> </ul> </li> <li>Specify the directory of <code>SymbolicRegression.jl</code> to PySR by setting <code>julia_project</code> in the <code>PySRRegressor</code> object, and run <code>.fit</code> when you're ready. That's it! No compilation or build steps required.<ul> <li>Note that it will automatically update your project by default; to turn this off, set <code>update=False</code>.</li> </ul> </li> </ol> <p>If you get comfortable enough with the backend, you might consider using the Julia package directly: the API is given on the SymbolicRegression.jl documentation.</p> <p>If you make a change that you think could be useful to other users, don't hesitate to open a pull request on either the PySR or SymbolicRegression.jl repositories! Contributions are very appreciated.</p>"},{"location":"examples/","title":"Toy Examples with Code","text":""},{"location":"examples/#preamble","title":"Preamble","text":"<pre><code>import numpy as np\nfrom pysr import *\n</code></pre>"},{"location":"examples/#1-simple-search","title":"1. Simple search","text":"<p>Here's a simple example where we find the expression <code>2 cos(x3) + x0^2 - 2</code>.</p> <pre><code>X = 2 * np.random.randn(100, 5)\ny = 2 * np.cos(X[:, 3]) + X[:, 0] ** 2 - 2\nmodel = PySRRegressor(binary_operators=[\"+\", \"-\", \"*\", \"/\"])\nmodel.fit(X, y)\nprint(model)\n</code></pre>"},{"location":"examples/#2-custom-operator","title":"2. Custom operator","text":"<p>Here, we define a custom operator and use it to find an expression:</p> <pre><code>X = 2 * np.random.randn(100, 5)\ny = 1 / X[:, 0]\nmodel = PySRRegressor(\nbinary_operators=[\"+\", \"*\"],\nunary_operators=[\"inv(x) = 1/x\"],\nextra_sympy_mappings={\"inv\": lambda x: 1/x},\n)\nmodel.fit(X, y)\nprint(model)\n</code></pre>"},{"location":"examples/#3-multiple-outputs","title":"3. Multiple outputs","text":"<p>Here, we do the same thing, but with multiple expressions at once, each requiring a different feature.</p> <pre><code>X = 2 * np.random.randn(100, 5)\ny = 1 / X[:, [0, 1, 2]]\nmodel = PySRRegressor(\nbinary_operators=[\"+\", \"*\"],\nunary_operators=[\"inv(x) = 1/x\"],\nextra_sympy_mappings={\"inv\": lambda x: 1/x},\n)\nmodel.fit(X, y)\n</code></pre>"},{"location":"examples/#4-plotting-an-expression","title":"4. Plotting an expression","text":"<p>For now, let's consider the expressions for output 0. We can see the LaTeX version of this with:</p> <pre><code>model.latex()[0]\n</code></pre> <p>or output 1 with <code>model.latex()[1]</code>.</p> <p>Let's plot the prediction against the truth:</p> <pre><code>from matplotlib import pyplot as plt\nplt.scatter(y[:, 0], model.predict(X)[:, 0])\nplt.xlabel('Truth')\nplt.ylabel('Prediction')\nplt.show()\n</code></pre> <p>Which gives us:</p> <p></p> <p>We may also plot the output of a particular expression by passing the index of the expression to <code>predict</code> (or <code>sympy</code> or <code>latex</code> as well)</p>"},{"location":"examples/#5-feature-selection","title":"5. Feature selection","text":"<p>PySR and evolution-based symbolic regression in general performs very poorly when the number of features is large. Even, say, 10 features might be too much for a typical equation search.</p> <p>If you are dealing with high-dimensional data with a particular type of structure, you might consider using deep learning to break the problem into smaller \"chunks\" which can then be solved by PySR, as explained in the paper 2006.11287.</p> <p>For tabular datasets, this is a bit trickier. Luckily, PySR has a built-in feature selection mechanism. Simply declare the parameter <code>select_k_features=5</code>, for selecting the most important 5 features.</p> <p>Here is an example. Let's say we have 30 input features and 300 data points, but only 2 of those features are actually used:</p> <pre><code>X = np.random.randn(300, 30)\ny = X[:, 3]**2 - X[:, 19]**2 + 1.5\n</code></pre> <p>Let's create a model with the feature selection argument set up:</p> <pre><code>model = PySRRegressor(\nbinary_operators=[\"+\", \"-\", \"*\", \"/\"],\nunary_operators=[\"exp\"],\nselect_k_features=5,\n)\n</code></pre> <p>Now let's fit this:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>Before the Julia backend is launched, you can see the string:</p> <pre><code>Using features ['x3', 'x5', 'x7', 'x19', 'x21']\n</code></pre> <p>which indicates that the feature selection (powered by a gradient-boosting tree) has successfully selected the relevant two features.</p> <p>This fit should find the solution quickly, whereas with the huge number of features, it would have struggled.</p> <p>This simple preprocessing step is enough to simplify our tabular dataset, but again, for more structured datasets, you should try the deep learning approach mentioned above.</p>"},{"location":"examples/#6-denoising","title":"6. Denoising","text":"<p>Many datasets, especially in the observational sciences, contain intrinsic noise. PySR is noise robust itself, as it is simply optimizing a loss function, but there are still some additional steps you can take to reduce the effect of noise.</p> <p>One thing you could do, which we won't detail here, is to create a custom log-likelihood given some assumed noise model. By passing weights to the fit function, and defining a custom loss function such as <code>loss=\"myloss(x, y, w) = w * (x - y)^2\"</code>, you can define any sort of log-likelihood you wish. (However, note that it must be bounded at zero)</p> <p>However, the simplest thing to do is preprocessing, just like for feature selection. To do this, set the parameter <code>denoise=True</code>. This will fit a Gaussian process (containing a white noise kernel) to the input dataset, and predict new targets (which are assumed to be denoised) from that Gaussian process.</p> <p>For example:</p> <pre><code>X = np.random.randn(100, 5)\nnoise = np.random.randn(100) * 0.1\ny = np.exp(X[:, 0]) + X[:, 1] + X[:, 2] + noise\n</code></pre> <p>Let's create and fit a model with the denoising argument set up:</p> <pre><code>model = PySRRegressor(\nbinary_operators=[\"+\", \"-\", \"*\", \"/\"],\nunary_operators=[\"exp\"],\ndenoise=True,\n)\nmodel.fit(X, y)\nprint(model)\n</code></pre> <p>If all goes well, you should find that it predicts the correct input equation, without the noise term!</p>"},{"location":"examples/#7-julia-packages-and-types","title":"7. Julia packages and types","text":"<p>PySR uses SymbolicRegression.jl as its search backend. This is a pure Julia package, and so can interface easily with any other Julia package. For some tasks, it may be necessary to load such a package.</p> <p>For example, let's say we wish to discovery the following relationship:</p> \\[ y = p_{3x + 1} - 5, \\] <p>where \\(p_i\\) is the \\(i\\)th prime number, and \\(x\\) is the input feature.</p> <p>Let's see if we can discover this using the Primes.jl package.</p> <p>First, let's manually initialize the Julia backend (here, with 8 threads and <code>-O3</code>):</p> <pre><code>import pysr\njl = pysr.julia_helpers.init_julia(julia_kwargs={\"threads\": 8, \"optimize\": 3})\n</code></pre> <p><code>jl</code> stores the Julia runtime.</p> <p>Now, let's run some Julia code to add the Primes.jl package to the PySR environment:</p> <pre><code>jl.eval(\"\"\"\nimport Pkg\nPkg.add(\"Primes\")\n\"\"\")\n</code></pre> <p>This imports the Julia package manager, and uses it to install <code>Primes.jl</code>. Now let's import <code>Primes.jl</code>:</p> <pre><code>jl.eval(\"import Primes\")\n</code></pre> <p>Now, we define a custom operator:</p> <pre><code>jl.eval(\"\"\"\nfunction p(i::T) where T\n    if (0.5 &lt; i &lt; 1000)\n        return T(Primes.prime(round(Int, i)))\n    else\n        return T(NaN)\n    end\nend\n\"\"\")\n</code></pre> <p>We have created a a function <code>p</code>, which takes an arbitrary number as input. <code>p</code> first checks whether the input is between 0.5 and 1000. If out-of-bounds, it returns <code>NaN</code>. If in-bounds, it rounds it to the nearest integer, compures the corresponding prime number, and then converts it to the same type as input.</p> <p>Next, let's generate a list of primes for our test dataset. Since we are using PyJulia, we can just call <code>p</code> directly to do this:</p> <pre><code>primes = {i: jl.p(i*1.0) for i in range(1, 999)}\n</code></pre> <p>Next, let's use this list of primes to create a dataset of \\(x, y\\) pairs:</p> <pre><code>import numpy as np\nX = np.random.randint(0, 100, 100)[:, None]\ny = [primes[3*X[i, 0] + 1] - 5 + np.random.randn()*0.001 for i in range(100)]\n</code></pre> <p>Note that we have also added a tiny bit of noise to the dataset.</p> <p>Finally, let's create a PySR model, and pass the custom operator. We also need to define the sympy equivalent, which we can leave as a placeholder for now:</p> <pre><code>from pysr import PySRRegressor\nimport sympy\nclass sympy_p(sympy.Function):\npass\nmodel = PySRRegressor(\nbinary_operators=[\"+\", \"-\", \"*\", \"/\"],\nunary_operators=[\"p\"],\nniterations=100,\nextra_sympy_mappings={\"p\": sympy_p}\n)\n</code></pre> <p>We are all set to go! Let's see if we can find the true relation:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>if all works out, you should be able to see the true relation (note that the constant offset might not be exactly 1, since it is allowed to round to the nearest integer). You can get the sympy version of the best equation with:</p> <pre><code>model.sympy()\n</code></pre>"},{"location":"examples/#8-complex-numbers","title":"8. Complex numbers","text":"<p>PySR can also search for complex-valued expressions. Simply pass data with a complex datatype (e.g., <code>np.complex128</code>), and PySR will automatically search for complex-valued expressions:</p> <pre><code>import numpy as np\nX = np.random.randn(100, 1) + 1j * np.random.randn(100, 1)\ny = (1 + 2j) * np.cos(X[:, 0] * (0.5 - 0.2j))\nmodel = PySRRegressor(\nbinary_operators=[\"+\", \"-\", \"*\"], unary_operators=[\"cos\"], niterations=100,\n)\nmodel.fit(X, y)\n</code></pre> <p>You can see that all of the learned constants are now complex numbers. We can get the sympy version of the best equation with:</p> <pre><code>model.sympy()\n</code></pre> <p>We can also make predictions normally, by passing complex data:</p> <pre><code>model.predict(X, -1)\n</code></pre> <p>to make predictions with the most accurate expression.</p>"},{"location":"examples/#9-custom-objectives","title":"9. Custom objectives","text":"<p>You can also pass a custom objectives as a snippet of Julia code, which might include symbolic manipulations or custom functional forms. These do not even need to be differentiable! First, let's look at the default objective used (a simplified version, without weights and with mean square error), so that you can see how to write your own:</p> <pre><code>function default_objective(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n(prediction, completion) = eval_tree_array(tree, dataset.X, options)\nif !completion\nreturn L(Inf)\nend\ndiffs = prediction .- dataset.y\nreturn sum(diffs .^ 2) / length(diffs)\nend\n</code></pre> <p>Here, the <code>where {T,L}</code> syntax defines the function for arbitrary types <code>T</code> and <code>L</code>. If you have <code>precision=32</code> (default) and pass in regular floating point data, then both <code>T</code> and <code>L</code> will be equal to <code>Float32</code>. If you pass in complex data, then <code>T</code> will be <code>ComplexF32</code> and <code>L</code> will be <code>Float32</code> (since we need to return a real number from the loss function). But, you don't need to worry about this, just make sure to return a scalar number of type <code>L</code>.</p> <p>The <code>tree</code> argument is the current expression being evaluated. You can read about the <code>tree</code> fields here.</p> <p>For example, let's fix a symbolic form of an expression, as a rational function. i.e., \\(P(X)/Q(X)\\) for polynomials \\(P\\) and \\(Q\\).</p> <pre><code>objective = \"\"\"\nfunction my_custom_objective(tree, dataset::Dataset{T,L}, options) where {T,L}\n    # Require root node to be binary, so we can split it,\n    # otherwise return a large loss:\n    tree.degree != 2 &amp;&amp; return L(Inf)\n    P = tree.l\n    Q = tree.r\n    # Evaluate numerator:\n    P_prediction, flag = eval_tree_array(P, dataset.X, options)\n    !flag &amp;&amp; return L(Inf)\n    # Evaluate denominator:\n    Q_prediction, flag = eval_tree_array(Q, dataset.X, options)\n    !flag &amp;&amp; return L(Inf)\n    # Impose functional form:\n    prediction = P_prediction ./ Q_prediction\n    diffs = prediction .- dataset.y\n    return sum(diffs .^ 2) / length(diffs)\nend\n\"\"\"\nmodel = PySRRegressor(\nniterations=100,\nbinary_operators=[\"*\", \"+\", \"-\"],\nfull_objective=objective,\n)\n</code></pre> <p>Warning: When using a custom objective like this that performs symbolic manipulations, many functionalities of PySR will not work, such as <code>.sympy()</code>, <code>.predict()</code>, etc. This is because the SymPy parsing does not know about how you are manipulating the expression, so you will need to do this yourself.</p> <p>Note how we did not pass <code>/</code> as a binary operator; it will just be implicit in the functional form.</p> <p>Let's generate an equation of the form \\(\\frac{x_0^2 x_1 - 2}{x_2^2 + 1}\\):</p> <pre><code>X = np.random.randn(1000, 3)\ny = (X[:, 0]**2 * X[:, 1] - 2) / (X[:, 2]**2 + 1)\n</code></pre> <p>Finally, let's fit:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>Note that the printed equation is not the same as the evaluated equation, because the printing functionality does not know about the functional form.</p> <p>We can get the string format with:</p> <pre><code>model.get_best().equation\n</code></pre> <p>(or, you could use <code>model.equations_.iloc[-1].equation</code>)</p> <p>For me, this equation was:</p> <pre><code>(((2.3554819 + -0.3554746) - (x1 * (x0 * x0))) - (-1.0000019 - (x2 * x2)))\n</code></pre> <p>looking at the bracket structure of the equation, we can see that the outermost bracket is split at the <code>-</code> operator (note that we ignore the root operator in the evaluation, as we simply evaluated each argument and divided the result) into <code>((2.3554819 + -0.3554746) - (x1 * (x0 * x0)))</code> and <code>(-1.0000019 - (x2 * x2))</code>, meaning that our discovered equation is equal to: \\(\\frac{x_0^2 x_1 - 2.0000073}{x_2^2 - 1.0000019}\\), which is nearly the same as the true equation!</p>"},{"location":"examples/#10-dimensional-constraints","title":"10. Dimensional constraints","text":"<p>One other feature we can exploit is dimensional analysis. Say that we know the physical units of each feature and output, and we want to find an expression that is dimensionally consistent.</p> <p>We can do this as follows, using <code>DynamicQuantities.jl</code> to assign units, passing a string specifying the units for each variable. First, let's make some data on Newton's law of gravitation, using astropy for units:</p> <pre><code>import numpy as np\nfrom astropy import units as u, constants as const\nM = (np.random.rand(100) + 0.1) * const.M_sun\nm = 100 * (np.random.rand(100) + 0.1) * u.kg\nr = (np.random.rand(100) + 0.1) * const.R_earth\nG = const.G\nF = G * M * m / r**2\n</code></pre> <p>We can see the units of <code>F</code> with <code>F.unit</code>.</p> <p>Now, let's create our model. Since this data has such a large dynamic range, let's also create a custom loss function that looks at the error in log-space:</p> <pre><code>loss = \"\"\"function loss_fnc(prediction, target)\n    scatter_loss = abs(log((abs(prediction)+1e-20) / (abs(target)+1e-20)))\n    sign_loss = 10 * (sign(prediction) - sign(target))^2\n    return scatter_loss + sign_loss\nend\n\"\"\"\n</code></pre> <p>Now let's define our model:</p> <pre><code>model = PySRRegressor(\nbinary_operators=[\"+\", \"-\", \"*\", \"/\"],\nunary_operators=[\"square\"],\nloss=loss,\ncomplexity_of_constants=2,\nmaxsize=25,\nniterations=100,\npopulations=50,\n# Amount to penalize dimensional violations:\ndimensional_constraint_penalty=10**5,\n)\n</code></pre> <p>and fit it, passing the unit information. To do this, we need to use the format of DynamicQuantities.jl.</p> <pre><code># Get numerical arrays to fit:\nX = pd.DataFrame(dict(\nM=M.to(\"M_sun\").value,\nm=m.to(\"kg\").value,\nr=r.to(\"R_earth\").value,\n))\ny = F.value\nmodel.fit(\nX,\ny,\nX_units=[\"Constants.M_sun\", \"kg\", \"Constants.R_earth\"],\ny_units=\"kg * m / s^2\"\n)\n</code></pre> <p>You can observe that all expressions with a loss under our penalty are dimensionally consistent! (The <code>\"[\u22c5]\"</code> indicates free units in a constant, which can cancel out other units in the expression.) For example,</p> <pre><code>\"y[m s\u207b\u00b2 kg] = (M[kg] * 2.6353e-22[\u22c5])\"\n</code></pre> <p>would indicate that the expression is dimensionally consistent, with a constant <code>\"2.6353e-22[m s\u207b\u00b2]\"</code>.</p> <p>Note that this expression has a large dynamic range so may be difficult to find. Consider searching with a larger <code>niterations</code> if needed.</p>"},{"location":"examples/#11-additional-features","title":"11. Additional features","text":"<p>For the many other features available in PySR, please read the Options section.</p>"},{"location":"interactive-docs/","title":"Interactive Reference \u2b50","text":"<p>The following docs are interactive, and, based on your selections, will create a snippet of Python code at the bottom which you can execute locally. Clicking on each parameter's name will display a description. Note that this is an incomplete list of options; for the full list, see the API Reference.</p> <p></p>"},{"location":"operators/","title":"Operators","text":""},{"location":"operators/#pre-defined","title":"Pre-defined","text":"<p>All Base julia operators that take 1 or 2 float32 as input, and output a float32 as output, are available. A selection of these and other valid operators are stated below.</p> <p>Binary</p> <p><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>^</code>, <code>greater</code>, <code>mod</code>, <code>logical_or</code>, <code>logical_and</code></p> <p>Unary</p> <p><code>neg</code>, <code>square</code>, <code>cube</code>, <code>exp</code>, <code>abs</code>, <code>log</code>, <code>log10</code>, <code>log2</code>, <code>log1p</code>, <code>sqrt</code>, <code>sin</code>, <code>cos</code>, <code>tan</code>, <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>atan</code>, <code>asinh</code>, <code>acosh</code>, <code>atanh_clip</code> (=atanh((x+1)%2 - 1)), <code>erf</code>, <code>erfc</code>, <code>gamma</code>, <code>relu</code>, <code>round</code>, <code>floor</code>, <code>ceil</code>, <code>round</code>, <code>sign</code>.</p>"},{"location":"operators/#custom","title":"Custom","text":"<p>Instead of passing a predefined operator as a string, you can define with by passing it to the <code>pysr</code> function, with, e.g.,</p> <pre><code>    PySRRegressor(\n...,\nunary_operators=[\"myfunction(x) = x^2\"],\nbinary_operators=[\"myotherfunction(x, y) = x^2*y\"]\n)\n</code></pre> <p>Make sure that it works with <code>Float32</code> as a datatype. That means you need to write <code>1.5f3</code> instead of <code>1.5e3</code>, if you write any constant numbers.</p> <p>Your operator should work with the entire real line (you can use abs(x) for operators requiring positive input - see <code>log_abs</code>); otherwise the search code will experience domain errors.</p>"},{"location":"options/","title":"Features and Options","text":"<p>Some configurable features and options in <code>PySR</code> which you may find useful include:</p> <ul> <li>Selecting from the accuracy-complexity curve</li> <li>Operators</li> <li>Number of outer search iterations</li> <li>Number of inner search iterations</li> <li>Multi-processing</li> <li>Populations</li> <li>Data weighting</li> <li>Max complexity and depth</li> <li>Mini-batching</li> <li>Variable names</li> <li>Constraining use of operators</li> <li>Custom complexities</li> <li>LaTeX and SymPy</li> <li>Exporting to numpy, pytorch, and jax</li> <li>Loss functions</li> <li>Model loading</li> </ul> <p>These are described below. Also check out the tuning page for workflow tips.</p> <p>The program will output a pandas DataFrame containing the equations to <code>PySRRegressor.equations</code> containing the loss value and complexity.</p> <p>It will also dump to a csv at the end of every iteration, which is <code>.hall_of_fame_{date_time}.csv</code> by default. It also prints the equations to stdout.</p>"},{"location":"options/#model-selection","title":"Model selection","text":"<p>By default, <code>PySRRegressor</code> uses <code>model_selection='best'</code> which selects an equation from <code>PySRRegressor.equations_</code> using a combination of accuracy and complexity. You can also select <code>model_selection='accuracy'</code>.</p> <p>By printing a model (i.e., <code>print(model)</code>), you can see the equation selection with the arrow shown in the <code>pick</code> column.</p>"},{"location":"options/#operators","title":"Operators","text":"<p>A list of operators can be found on the operators page. One can define custom operators in Julia by passing a string:</p> <pre><code>PySRRegressor(niterations=100,\nbinary_operators=[\"mult\", \"plus\", \"special(x, y) = x^2 + y\"],\nextra_sympy_mappings={'special': lambda x, y: x**2 + y},\nunary_operators=[\"cos\"])\n</code></pre> <p>Now, the symbolic regression code can search using this <code>special</code> function that squares its left argument and adds it to its right. Make sure all passed functions are valid Julia code, and take one (unary) or two (binary) float32 scalars as input, and output a float32. This means if you write any real constants in your operator, like <code>2.5</code>, you have to write them instead as <code>2.5f0</code>, which defines it as <code>Float32</code>. Operators are automatically vectorized.</p> <p>One should also define <code>extra_sympy_mappings</code>, so that the SymPy code can understand the output equation from Julia, when constructing a useable function. This step is optional, but is necessary for the <code>lambda_format</code> to work.</p>"},{"location":"options/#iterations","title":"Iterations","text":"<p>This is the total number of generations that <code>pysr</code> will run for. I usually set this to a large number, and exit when I am satisfied with the equations.</p>"},{"location":"options/#cycles-per-iteration","title":"Cycles per iteration","text":"<p>Each cycle considers every 10-equation subsample (re-sampled for each individual 10, unless <code>fast_cycle</code> is set in which case the subsamples are separate groups of equations) a single time, producing one mutated equation for each. The parameter <code>ncyclesperiteration</code> defines how many times this occurs before the equations are compared to the hall of fame, and new equations are migrated from the hall of fame, or from other populations. It also controls how slowly annealing occurs. You may find that increasing <code>ncyclesperiteration</code> results in a higher cycles-per-second, as the head worker needs to reduce and distribute new equations less often, and also increases diversity. But at the same time, a smaller number it might be that migrating equations from the hall of fame helps each population stay closer to the best current equations.</p>"},{"location":"options/#processors","title":"Processors","text":"<p>One can adjust the number of workers used by Julia with the <code>procs</code> option. You should set this equal to the number of cores you want <code>pysr</code> to use.</p>"},{"location":"options/#populations","title":"Populations","text":"<p>By default, <code>populations=15</code>, but you can set a different number of populations with this option. More populations may increase the diversity of equations discovered, though will take longer to train. However, it is usually more efficient to have <code>populations&gt;procs</code>, as there are multiple populations running on each core.</p>"},{"location":"options/#weighted-data","title":"Weighted data","text":"<p>Here, we assign weights to each row of data using inverse uncertainty squared. We also use 10 processes for the search instead of the default.</p> <pre><code>sigma = ...\nweights = 1/sigma**2\nmodel = PySRRegressor(procs=10)\nmodel.fit(X, y, weights=weights)\n</code></pre>"},{"location":"options/#max-size","title":"Max size","text":"<p><code>maxsize</code> controls the maximum size of equation (number of operators, constants, variables). <code>maxdepth</code> is by default not used, but can be set to control the maximum depth of an equation. These will make processing faster, as longer equations take longer to test.</p> <p>One can warm up the maxsize from a small number to encourage PySR to start simple, by using the <code>warmupMaxsize</code> argument. This specifies that maxsize increases every <code>warmupMaxsize</code>.</p>"},{"location":"options/#batching","title":"Batching","text":"<p>One can turn on mini-batching, with the <code>batching</code> flag, and control the batch size with <code>batch_size</code>. This will make evolution faster for large datasets. Equations are still evaluated on the entire dataset at the end of each iteration to compare to the hall of fame, but only on a random subset during mutations and annealing.</p>"},{"location":"options/#variable-names","title":"Variable Names","text":"<p>You can pass a list of strings naming each column of <code>X</code> with <code>variable_names</code>. Alternatively, you can pass <code>X</code> as a pandas dataframe and the columns will be used as variable names. Make sure only alphabetical characters and <code>_</code> are used in these names.</p>"},{"location":"options/#constraining-use-of-operators","title":"Constraining use of operators","text":"<p>One can limit the complexity of specific operators with the <code>constraints</code> parameter. There is a \"maxsize\" parameter to PySR, but there is also an operator-level \"constraints\" parameter. One supplies a dict, like so:</p> <pre><code>constraints={'pow': (-1, 1), 'mult': (3, 3), 'cos': 5}\n</code></pre> <p>What this says is that: a power law \\(x^y\\) can have an expression of arbitrary (-1) complexity in the x, but only complexity 1 (e.g., a constant or variable) in the y. So \\((x_0 + 3)^{5.5}\\) is allowed, but \\(5.5^{x_0 + 3}\\) is not. I find this helps a lot for getting more interpretable equations. The other terms say that each multiplication can only have sub-expressions of up to complexity 3 (e.g., \\(5.0 + x_2\\)) in each side, and cosine can only operate on expressions of complexity 5 (e.g., \\(5.0 + x_2 exp(x_3)\\)).</p>"},{"location":"options/#custom-complexity","title":"Custom complexity","text":"<p>By default, all operators, constants, and instances of variables have a complexity of 1. The sum of the complexities of all terms is the total complexity of an expression. You may change this by configuring the options:</p> <ul> <li><code>complexity_of_operators</code> - pass a dictionary of <code>&lt;str&gt;: &lt;int&gt;</code> pairs   to change the complexity of each operator. If an operator is not   specified, it will have the default complexity of 1.</li> <li><code>complexity_of_constants</code> - supplying an integer will make all constants   have that complexity.</li> <li><code>complexity_of_variables</code> - supplying an integer will make all variables   have that complexity.</li> </ul>"},{"location":"options/#latex-and-sympy","title":"LaTeX and SymPy","text":"<p>After running <code>model.fit(...)</code>, you can look at <code>model.equations</code> which is a pandas dataframe. The <code>sympy_format</code> column gives sympy equations, and the <code>lambda_format</code> gives callable functions. You can optionally pass a pandas dataframe to the callable function, if you called <code>.fit</code> on a pandas dataframe as well.</p> <p>There are also some helper functions for doing this quickly.</p> <ul> <li><code>model.latex()</code> will generate a TeX formatted output of your equation.</li> <li><code>model.latex_table(indices=[2, 5, 8])</code> will generate a formatted LaTeX table including all the specified equations.</li> <li><code>model.sympy()</code> will return the SymPy representation.</li> <li><code>model.jax()</code> will return a callable JAX function combined with parameters (see below)</li> <li><code>model.pytorch()</code> will return a PyTorch model (see below).</li> </ul>"},{"location":"options/#exporting-to-numpy-pytorch-and-jax","title":"Exporting to numpy, pytorch, and jax","text":"<p>By default, the dataframe of equations will contain columns with the identifier <code>lambda_format</code>. These are simple functions which correspond to the equation, but executed with numpy functions. You can pass your <code>X</code> matrix to these functions just as you did to the <code>model.fit</code> call. Thus, this allows you to numerically evaluate the equations over different output.</p> <p>Calling <code>model.predict</code> will execute the <code>lambda_format</code> of the best equation, and return the result. If you selected <code>model_selection=\"best\"</code>, this will use an equation that combines accuracy with simplicity. For <code>model_selection=\"accuracy\"</code>, this will just look at accuracy.</p> <p>One can do the same thing for PyTorch, which uses code from sympytorch, and for JAX, which uses code from sympy2jax.</p> <p>Calling <code>model.pytorch()</code> will return a PyTorch module which runs the equation, using PyTorch functions, over <code>X</code> (as a PyTorch tensor). This is differentiable, and the parameters of this PyTorch module correspond to the learned parameters in the equation, and are trainable.</p> <pre><code>torch_model = model.pytorch()\ntorch_model(X)\n</code></pre> <p>Warning: If you are using custom operators, you must define <code>extra_torch_mappings</code> or <code>extra_jax_mappings</code> (both are <code>dict</code> of callables) to provide an equivalent definition of the functions. (At any time you can set these parameters or any others with <code>model.set_params</code>.)</p> <p>For JAX, you can equivalently call <code>model.jax()</code> This will return a dictionary containing a <code>'callable'</code> (a JAX function), and <code>'parameters'</code> (a list of parameters in the equation). You can execute this function with:</p> <pre><code>jax_model = model.jax()\njax_model['callable'](X, jax_model['parameters'])\n</code></pre> <p>Since the parameter list is a jax array, this therefore lets you also train the parameters within JAX (and is differentiable).</p>"},{"location":"options/#loss","title":"<code>loss</code>","text":"<p>The default loss is mean-square error, and weighted mean-square error. One can pass an arbitrary Julia string to define a custom loss, using, e.g., <code>loss=\"myloss(x, y) = abs(x - y)^1.5\"</code>. For more details, see the Losses page for SymbolicRegression.jl.</p> <p>Here are some additional examples:</p> <p>abs(x-y) loss</p> <pre><code>PySRRegressor(..., loss=\"f(x, y) = abs(x - y)^1.5\")\n</code></pre> <p>Note that the function name doesn't matter:</p> <pre><code>PySRRegressor(..., loss=\"loss(x, y) = abs(x * y)\")\n</code></pre> <p>With weights:</p> <pre><code>model = PySRRegressor(..., loss=\"myloss(x, y, w) = w * abs(x - y)\") \nmodel.fit(..., weights=weights)\n</code></pre> <p>Weights can be used in arbitrary ways:</p> <pre><code>model = PySRRegressor(..., weights=weights, loss=\"myloss(x, y, w) = abs(x - y)^2/w^2\")\nmodel.fit(..., weights=weights)\n</code></pre> <p>Built-in loss (faster) (see losses). This one computes the L3 norm:</p> <pre><code>PySRRegressor(..., loss=\"LPDistLoss{3}()\")\n</code></pre> <p>Can also uses these losses for weighted (weighted-average):</p> <pre><code>model = PySRRegressor(..., weights=weights, loss=\"LPDistLoss{3}()\")\nmodel.fit(..., weights=weights)\n</code></pre>"},{"location":"options/#model-loading","title":"Model loading","text":"<p>PySR will automatically save a pickle file of the model state when you call <code>model.fit</code>, once before the search starts, and again after the search finishes. The filename will have the same base name as the input file, but with a <code>.pkl</code> extension. You can load the saved model state with:</p> <pre><code>model = PySRRegressor.from_file(pickle_filename)\n</code></pre> <p>If you have a long-running job and would like to load the model before completion, you can also do this. In this case, the model loading will use the <code>csv</code> file to load the equations, since the <code>csv</code> file is continually updated during the search. Once the search completes, the model including its equations will be saved to the pickle file, overwriting the existing version.</p>"},{"location":"papers/","title":"Research","text":"<p>Below is a showcase of papers which have used PySR to discover or rediscover a symbolic model. These are sorted by the date of release, with most recent papers at the top.</p> <p>If you have used PySR in your research, please submit a pull request to add your paper to this file.</p> <p></p> <p> Arthur Grundner 1,2, Tom Beucler 3, Pierre Gentine 2,3, Veronika Eyring 1,4 <p>1Institut f\u00fcr Physik der Atmosph\u00e4re, Deutsches Zentrum f\u00fcr Luft- und Raumfahrt, 2Center for Learning the Earth with Artificial Intelligence And Physics, Columbia University, 3Institute of Earth Surface Dynamics, University of Lausanne, 4Institute of Environmental Physics, University of Bremen </p> <p>Abstract: A promising method for improving the representation of clouds in climate models, and hence climate projections, is to develop machine learning-based parameterizations using output from global storm-resolving models. While neural networks can achieve state-of-the-art performance, they are typically climate model-specific, require post-hoc tools for interpretation, and struggle to predict outside of their training distribution. To avoid these limitations, we combine symbolic regression, sequential feature selection, and physical constraints in a hierarchical modeling framework. This framework allows us to discover new equations diagnosing cloud cover from coarse-grained variables of global storm-resolving model simulations. These analytical equations are interpretable by construction and easily transferable to other grids or climate models. Our best equation balances performance and complexity, achieving a performance comparable to that of neural networks (\\(R^2=0.94\\)) while remaining simple (with only 13 trainable parameters). It reproduces cloud cover distributions more accurately than the Xu-Randall scheme across all cloud regimes (Hellinger distances \\(&lt;0.09\\)), and matches neural networks in condensate-rich regimes. When applied and fine-tuned to the ERA5 reanalysis, the equation exhibits superior transferability to new data compared to all other optimal cloud cover schemes. Our findings demonstrate the effectiveness of symbolic regression in discovering interpretable, physically-consistent, and nonlinear equations to parameterize cloud cover.</p> <p></p> <p> Yanzhang Li 1, Hongyu Wang 2, Yan Li 1, Xiangzhi Bai 2, Anhuai Lu 1 <p>1Peking University, 2Beihang University </p> <p>Abstract: Electron transfer is the most elementary process in nature, but the existing electron transfer rules are seldom applied to high-pressure situations, such as in the deep Earth. Here we show a deep learning model to obtain the electronegativity of 96 elements under arbitrary pressure, and a regressed unified formula to quantify its relationship with pressure and electronic configuration. The relative work function of minerals is further predicted by electronegativity, presenting a decreasing trend with pressure because of pressure-induced electron delocalization. Using the work function as the case study of electronegativity, it reveals that the driving force behind directional electron transfer results from the enlarged work function difference between compounds with pressure. This well explains the deep high-conductivity anomalies, and helps discover the redox reactivity between widespread Fe(II)-bearing minerals and water during ongoing subduction. Our results give an insight into the fundamental physicochemical properties of elements and their compounds under pressure</p> <p></p> <p> Digvijay Wadekar 1, Leander Thiele 2, J. Colin Hill 3, Shivam Pandey 4, Francisco Villaescusa-Navarro 5, David N. Spergel 5, Miles Cranmer 2, Daisuke Nagai 6, Daniel Angl\u00e9s-Alc\u00e1zar 7, Shirley Ho 5, Lars Hernquist 8 <p>1Institute for Advanced Study, 2Princeton University, 3Columbia University, 4University of Pennsylvania, 5Flatiron Institute, 6Yale University, 7University of Connecticut, 8Harvard University </p> <p>Abstract: Ionized gas in the halo circumgalactic medium leaves an imprint on the cosmic microwave background via the thermal Sunyaev-Zeldovich (tSZ) effect. Feedback from active galactic nuclei (AGN) and supernovae can affect the measurements of the integrated tSZ flux of halos (\\(Y_{SZ}\\)) and cause its relation with the halo mass (\\(Y_{SZ}-M\\)) to deviate from the self-similar power-law prediction of the virial theorem. We perform a comprehensive study of such deviations using CAMELS, a suite of hydrodynamic simulations with extensive variations in feedback prescriptions. We use a combination of two machine learning tools (random forest and symbolic regression) to search for analogues of the \\(Y-M\\) relation which are more robust to feedback processes for low masses (\\(M \\leq 10^{14} M_{\\odot}/h\\)); we find that simply replacing \\(Y \\rightarrow Y(1+M_\\ast/M_{\\text{gas}})\\) in the relation makes it remarkably self-similar. This could serve as a robust multiwavelength mass proxy for low-mass clusters and galaxy groups. Our methodology can also be generally useful to improve the domain of validity of other astrophysical scaling relations. We also forecast that measurements of the Y-M relation could provide percent-level constraints on certain combinations of feedback parameters and/or rule out a major part of the parameter space of supernova and AGN feedback models used in current state-of-the-art hydrodynamic simulations. Our results can be useful for using upcoming SZ surveys (e.g. SO, CMB-S4) and galaxy surveys (e.g. DESI and Rubin) to constrain the nature of baryonic feedback. Finally, we find that the an alternative relation, \\(Y-M_{\\ast}\\), provides complementary information on feedback than \\(Y-M\\).</p> <p></p> <p> Sergiy Verstyuk 1, Michael R. Douglas 1 <p>1Harvard University </p> <p>Abstract: Machine learning (ML) is becoming more and more important throughout the mathematical and theoretical sciences. In this work we apply modern ML methods to gravity models of pairwise interactions in international economics. We explain the formulation of graphical neural networks (GNNs), models for graph-structured data that respect the properties of exchangeability and locality. GNNs are a natural and theoretically appealing class of models for international trade, which we demonstrate empirically by fitting them to a large panel of annual-frequency country-level data. We then use a symbolic regression algorithm to turn our fits into interpretable models with performance comparable to state of the art hand-crafted models motivated by economic theory. The resulting symbolic models contain objects resembling market access functions, which were developed in modern structural literature, but in our analysis arise ab initio without being explicitly postulated. Along the way, we also produce several model-consistent and model-agnostic ML-based measures of bilateral trade accessibility.</p> <p></p> <p> Pablo Lemos 1,2, Niall Jeffrey 3,2, Miles Cranmer 4, Shirley Ho 4,5,6,7, Peter Battaglia 8 <p>1University of Sussex, 2University College London, 3ENS, 4Princeton University, 5Flatiron Institute, 6Carnegie Mellon University, 7New York University, 8DeepMind </p> <p>Abstract: We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a \"graph neural network\" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery.</p> <p></p> <p> Patrick Kidger 1 <p>1University of Oxford </p> <p>Abstract: The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.</p> <p></p> <p> Digvijay Wadekar 1, Leander Thiele 2, Francisco Villaescusa-Navarro 3, J. Colin Hill 4, Miles Cranmer 2, David N. Spergel 3, Nicholas Battaglia 5, Daniel Angl\u00e9s-Alc\u00e1zar 6, Lars Hernquist 7, Shirley Ho 3 <p>1Institute for Advanced Study, 2Princeton University, 3Flatiron Institute, 4Columbia University, 5Cornell University, 6University of Connecticut, 7Harvard University </p> <p>Abstract: Complex systems (stars, supernovae, galaxies, and clusters) often exhibit low scatter relations between observable properties (e.g., luminosity, velocity dispersion, oscillation period, temperature). These scaling relations can illuminate the underlying physics and can provide observational tools for estimating masses and distances. Machine learning can provide a fast and systematic way to search for new scaling relations (or for simple extensions to existing relations) in abstract high-dimensional parameter spaces. We use a machine learning tool called symbolic regression (SR), which models the patterns in a given dataset in the form of analytic equations. We focus on the Sunyaev-Zeldovich flux-cluster mass relation (Y-M), the scatter in which affects inference of cosmological parameters from cluster abundance data. Using SR on the data from the IllustrisTNG hydrodynamical simulation, we find a new proxy for cluster mass which combines \\(Y_{SZ}\\) and concentration of ionized gas (cgas): \\(M \\propto Y_{\\text{conc}}^{3/5} \\equiv Y_{SZ}^{3/5} (1 - A c_\\text{gas})\\). Yconc reduces the scatter in the predicted M by ~ 20 - 30% for large clusters (\\(M &gt; 10^{14} M_{\\odot}/h\\)) at both high and low redshifts, as compared to using just \\(Y_{SZ}\\). We show that the dependence on cgas is linked to cores of clusters exhibiting larger scatter than their outskirts. Finally, we test Yconc on clusters from simulations of the CAMELS project and show that Yconc is robust against variations in cosmology, astrophysics, subgrid physics, and cosmic variance. Our results and methodology can be useful for accurate multiwavelength cluster mass estimation from current and upcoming CMB and X-ray surveys like ACT, SO, SPT, eROSITA and CMB-S4.</p> <p></p> <p> Ana Maria Delgado 1, Digvijay Wadekar 2,3, Boryana Hadzhiyska 1, Sownak Bose 1,7, Lars Hernquist 1, Shirley Ho 2,4,5,6 <p>1Center for Astrophysics | Harvard &amp; Smithsonian, 2New York University, 3Institute for Advanced Study, 4Flatiron Institute, 5Princeton University, 6Carnegie Mellon University, 7Durham University </p> <p>Abstract: To extract information from the clustering of galaxies on non-linear scales, we need to model the connection between galaxies and halos accurately and in a flexible manner. Standard halo occupation distribution (HOD) models make the assumption that the galaxy occupation in a halo is a function of only its mass, however, in reality, the occupation can depend on various other parameters including halo concentration, assembly history, environment, spin, etc. Using the IllustrisTNG hydrodynamic simulation as our target, we show that machine learning tools can be used to capture this high-dimensional dependence and provide more accurate galaxy occupation models. Specifically, we use a random forest regressor to identify which secondary halo parameters best model the galaxy-halo connection and symbolic regression to augment the standard HOD model with simple equations capturing the dependence on those parameters, namely the local environmental overdensity and shear, at the location of a halo. This not only provides insights into the galaxy-formation relationship but, more importantly, improves the clustering statistics of the modeled galaxies significantly. Our approach demonstrates that machine learning tools can help us better understand and model the galaxy-halo connection, and are therefore useful for galaxy formation and cosmology studies from upcoming galaxy surveys.</p> <p></p> <p> Anja Butter 1, Tilman Plehn 1, Nathalie Soybelman 1, Johann Brehmer 2 <p>1Institut fur Theoretische Physik, Universitat Heidelberg, 2Center for Data Science, New York University </p> <p>Abstract: While neural networks offer an attractive way to numerically encode functions, actual formulas remain the language of theoretical particle physics. We show how symbolic regression trained on matrix-element information provides, for instance, optimal LHC observables in an easily interpretable form. We introduce the method using the effect of a dimension-6 coefficient on associated ZH production. We then validate it for the known case of CP-violation in weak-boson-fusion Higgs production, including detector effects.</p> <p></p> <p> Helen Shao 1, Francisco Villaescusa-Navarro 1,2, Shy Genel 2,3, David N. Spergel 2,1, Daniel Angles-Alcazar 4,2, Lars Hernquist 5, Romeel Dave 6,7,8, Desika Narayanan 9,10, Gabriella Contardo 2, Mark Vogelsberger 11 <p>1Princeton University, 2Flatiron Institute, 3Columbia University, 4University of Connecticut, 5Center for Astrophysics | Harvard &amp; Smithsonian, 6University of Edinburgh, 7University of the Western Cape, 8South African Astronomical Observatories, 9University of Florida, 10University of Florida Informatics Institute, 11MIT </p> <p>Abstract: We use a generic formalism designed to search for relations in high-dimensional spaces to determine if the total mass of a subhalo can be predicted from other internal properties such as velocity dispersion, radius, or star-formation rate. We train neural networks using data from the Cosmology and Astrophysics with MachinE Learning Simulations (CAMELS) project and show that the model can predict the total mass of a subhalo with high accuracy: more than 99% of the subhalos have a predicted mass within 0.2 dex of their true value. The networks exhibit surprising extrapolation properties, being able to accurately predict the total mass of any type of subhalo containing any kind of galaxy at any redshift from simulations with different cosmologies, astrophysics models, subgrid physics, volumes, and resolutions, indicating that the network may have found a universal relation. We then use different methods to find equations that approximate the relation found by the networks and derive new analytic expressions that predict the total mass of a subhalo from its radius, velocity dispersion, and maximum circular velocity. We show that in some regimes, the analytic expressions are more accurate than the neural networks. We interpret the relation found by the neural network and approximated by the analytic equation as being connected to the virial theorem.</p> <p></p> <p> Jessica Craven 1, Vishnu Jejjala 1, Arjun Kar 2 <p>1University of the Witwatersrand, 2University of British Columbia </p> <p>Abstract: We present a simple phenomenological formula which approximates the hyperbolic volume of a knot using only a single evaluation of its Jones polynomial at a root of unity. The average error is just 2.86% on the first 1.7 million knots, which represents a large improvement over previous formulas of this kind. To find the approximation formula, we use layer-wise relevance propagation to reverse engineer a black box neural network which achieves a similar average error for the same approximation task when trained on 10% of the total dataset. The particular roots of unity which appear in our analysis cannot be written as e2\u03c0i/(k+2) with integer k; therefore, the relevant Jones polynomial evaluations are not given by unknot-normalized expectation values of Wilson loop operators in conventional SU(2) Chern-Simons theory with level k. Instead, they correspond to an analytic continuation of such expectation values to fractional level. We briefly review the continuation procedure and comment on the presence of certain Lefschetz thimbles, to which our approximation formula is sensitive, in the analytically continued Chern-Simons integration cycle.</p> <p></p> <p> Digvijay Wadekar 1, Francisco Villaescusa-Navarro 2,3, Shirley Ho 2,3,4, Laurence Perreault-Levasseur 3,5,6 <p>1New York University, 2Princeton University, 3Flatiron Institute, 4Carnegie Mellon University, 5Universit\u00e9 de Montr\u00e9al, 6Mila </p> <p>Abstract: Upcoming 21cm surveys will map the spatial distribution of cosmic neutral hydrogen (HI) over unprecedented volumes. Mock catalogues are needed to fully exploit the potential of these surveys. Standard techniques employed to create these mock catalogs, like Halo Occupation Distribution (HOD), rely on assumptions such as the baryonic properties of dark matter halos only depend on their masses. In this work, we use the state-of-the-art magneto-hydrodynamic simulation IllustrisTNG to show that the HI content of halos exhibits a strong dependence on their local environment. We then use machine learning techniques to show that this effect can be 1) modeled by these algorithms and 2) parametrized in the form of novel analytic equations. We provide physical explanations for this environmental effect and show that ignoring it leads to underprediction of the real-space 21-cm power spectrum at k\u22730.05 h/Mpc by \u227310%, which is larger than the expected precision from upcoming surveys on such large scales. Our methodology of combining numerical simulations with machine learning techniques is general, and opens a new direction at modeling and parametrizing the complex physics of assembly bias needed to generate accurate mocks for galaxy and line intensity mapping surveys.</p>"},{"location":"papers/#data-driven-equation-discovery-of-a-cloud-cover-parameterization","title":"Data-Driven Equation Discovery of a Cloud Cover Parameterization","text":""},{"location":"papers/#electron-transfer-rules-of-minerals-under-pressure-informed-by-machine-learning","title":"Electron Transfer Rules of Minerals under Pressure informed by Machine Learning","text":""},{"location":"papers/#the-sz-flux-mass-y-m-relation-at-low-halo-masses-improvements-with-symbolic-regression-and-strong-constraints-on-baryonic-feedback","title":"The SZ flux-mass (Y-M) relation at low halo masses: improvements with symbolic regression and strong constraints on baryonic feedback","text":""},{"location":"papers/#machine-learning-the-gravity-equation-for-international-trade","title":"Machine Learning the Gravity Equation for International Trade","text":""},{"location":"papers/#rediscovering-orbital-mechanics-with-machine-learning","title":"Rediscovering orbital mechanics with machine learning","text":""},{"location":"papers/#thesis-on-neural-differential-equations-section-61","title":"(Thesis) On Neural Differential Equations - Section 6.1","text":""},{"location":"papers/#augmenting-astrophysical-scaling-relations-with-machine-learning-application-to-reducing-the-sz-flux-mass-scatter","title":"Augmenting astrophysical scaling relations with machine learning: application to reducing the SZ flux-mass scatter","text":""},{"location":"papers/#modeling-the-galaxy-halo-connection-with-machine-learning","title":"Modeling the galaxy-halo connection with machine learning","text":""},{"location":"papers/#back-to-the-formula-lhc-edition","title":"Back to the Formula -- LHC Edition","text":""},{"location":"papers/#finding-universal-relations-in-subhalo-properties-with-artificial-intelligence","title":"Finding universal relations in subhalo properties with artificial intelligence","text":""},{"location":"papers/#disentangling-a-deep-learned-volume-formula","title":"Disentangling a deep learned volume formula","text":""},{"location":"papers/#modeling-assembly-bias-with-machine-learning-and-symbolic-regression","title":"Modeling assembly bias with machine learning and symbolic regression","text":""},{"location":"tuning/","title":"Tuning and Workflow Tips","text":"<p>I give a short guide below on how I like to tune PySR for my applications.</p> <p>First, my general tips would be to avoid using redundant operators, like how <code>pow</code> can do the same things as <code>square</code>, or how <code>-</code> (binary) and <code>neg</code> (unary) are equivalent. The fewer operators the better! Only use operators you need.</p> <p>When running PySR, I usually do the following:</p> <p>I run from IPython (Jupyter Notebooks don't work as well1) on the head node of a slurm cluster. Passing <code>cluster_manager=\"slurm\"</code> will make PySR set up a run over the entire allocation. I set <code>procs</code> equal to the total number of cores over my entire allocation.</p> <ol> <li>Use the default parameters.</li> <li>Use only the operators I think it needs and no more.</li> <li>Set <code>niterations</code> to some very large value, so it just runs for a week until my job finishes. If the equation looks good, I quit the job early.</li> <li>Increase <code>populations</code> to <code>3*num_cores</code>.</li> <li>Set <code>ncyclesperiteration</code> to maybe <code>5000</code> or so, until the head node occupation is under <code>10%</code>.</li> <li>Set <code>constraints</code> and <code>nested_constraints</code> as strict as possible. These can help quite a bit with exploration. Typically, if I am using <code>pow</code>, I would set <code>constraints={\"pow\": (9, 1)}</code>, so that power laws can only have a variable or constant as their exponent. If I am using <code>sin</code> and <code>cos</code>, I also like to set <code>nested_constraints={\"sin\": {\"sin\": 0, \"cos\": 0}, \"cos\": {\"sin\": 0, \"cos\": 0}}</code>, so that sin and cos can't be nested, which seems to happen frequently. (Although in practice I would just use <code>sin</code>, since the search could always add a phase offset!)</li> <li>Set <code>maxsize</code> a bit larger than the final size you want. e.g., if you want a final equation of size <code>30</code>, you might set this to <code>35</code>, so that it has a bit of room to explore.</li> <li>Set <code>maxdepth</code> strictly, but leave a bit of room for exploration. e.g., if you want a final equation limited to a depth of <code>5</code>, you might set this to <code>6</code> or <code>7</code>, so that it has a bit of room to explore. </li> <li>Set <code>parsimony</code> equal to about the minimum loss you would expect, divided by 5-10. e.g., if you expect the final equation to have a loss of <code>0.001</code>, you might set <code>parsimony=0.0001</code>.</li> <li>Set <code>weight_optimize</code> to some larger value, maybe <code>0.001</code>. This is very important if <code>ncyclesperiteration</code> is large, so that optimization happens more frequently.</li> <li>Set <code>turbo</code> to <code>True</code>. This may or not work, if there's an error just turn it off (some operators are not SIMD-capable). If it does work, it should give you a nice 20% speedup.</li> </ol> <p>Since I am running in IPython, I can just hit <code>q</code> and then <code>&lt;enter&gt;</code> to stop the job, tweak the hyperparameters, and then start the search again. I can also use <code>warm_start=True</code> if I wish to continue where I left off (though note that changing some parameters, like <code>maxsize</code>, are incompatible with warm starts).</p> <p>Some things I try out to see if they help:</p> <ol> <li>Play around with <code>complexity_of_operators</code>. Set operators you dislike (e.g., <code>pow</code>) to have a larger complexity.</li> <li>Try setting <code>adaptive_parsimony_scaling</code> a bit larger, maybe up to <code>1000</code>.</li> <li>Sometimes I try using <code>warmup_maxsize_by</code>. This is useful if you find that the search finds a very complex equation very quickly, and then gets stuck. It basically forces it to start at the simpler equations and build up complexity slowly.</li> <li>Play around with different losses:<ul> <li>I typically try <code>L2DistLoss()</code> and <code>L1DistLoss()</code>. L1 loss is more robust to outliers compared to L2 (L1 finds the median, while L2 finds the mean of a random variable), so is often a good choice for a noisy dataset. </li> <li>I might also provide the <code>weights</code> parameter to <code>fit</code> if there is some reasonable choice of weighting. For example, maybe I know the signal-to-noise of a particular row of <code>y</code> - I would set that SNR equal to the weights. Or, perhaps I do some sort of importance sampling, and weight the rows by importance.</li> </ul> </li> </ol> <p>Very rarely I might also try tuning the mutation weights, the crossover probability, or the optimization parameters. I never use <code>denoise</code> or <code>select_k_features</code> as I find they aren't very useful.</p> <p>For large datasets I usually just randomly sample ~1000 points or so. In case all the points matter, I might use <code>batching=True</code>.</p> <p>If I find the equations get very complex and I'm not sure if they are numerically precise, I might set <code>precision=64</code>.</p> <p>Once a run is finished, I use the <code>PySRRegressor.from_file</code> function to load the saved search in a different process (requires the pickle file, and possibly also the <code>.csv</code> file if you quit early). I can then explore the equations, convert them to LaTeX, and plot their output.</p>"},{"location":"tuning/#more-tips","title":"More Tips","text":"<p>You might also wish to explore the discussions page for more tips, and to see if anyone else has had similar questions. Be sure to also read through the reference.</p> <ol> <li> <p>Jupyter Notebooks are supported by PySR, but miss out on some useful features available in IPython and Python: the progress bar, and early stopping with \"q\". In Jupyter you cannot interrupt a search once it has started; you have to restart the kernel. See this issue for updates.\u00a0\u21a9</p> </li> </ol>"}]}